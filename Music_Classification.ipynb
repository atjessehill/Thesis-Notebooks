{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Music Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPuKyD4FXvGenhO47p4xGaV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atjessehill/Thesis-Notebooks/blob/main/Music_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kss43FItad0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ece17c14-411a-40ea-f6b5-aefe7cb3cd91"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "from os import path\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.keras as keras\n",
        "from sklearn import preprocessing\n",
        "import statistics\n",
        "from keras import backend as K\n",
        "from keras import Model\n",
        "import scipy\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lxhc1sWKn0hp"
      },
      "source": [
        "keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6MOaIBBMs_m"
      },
      "source": [
        "BASE = 'drive/My Drive'\n",
        "SONG_SAMPLE_PATH = 'Thesis/Samples'\n",
        "DATA_SAVE_PATH = 'Thesis/InputData'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9JeCjXPJGfu"
      },
      "source": [
        "\"\"\" \n",
        "\n",
        "Mostly unneeded code at this point, this was from earlier experiments using \n",
        "matrix of MFCC's, which needed to be rescaled.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def load_npy(path, fold):\n",
        "  x_train = np.load(os.path.join(path,'x_train_'+str(fold)+'.npy'))\n",
        "  x_test = np.load(os.path.join(path,'x_test_'+str(fold)+'.npy'))\n",
        "  y_train = np.load(os.path.join(path,'y_train_'+str(fold)+'.npy'))\n",
        "  y_test = np.load(os.path.join(path,'y_test_'+str(fold)+'.npy'))\n",
        "  x_train = x_train[..., np.newaxis]\n",
        "  x_test = x_test[..., np.newaxis]\n",
        "  return x_train, x_test, y_train, y_test\n",
        "  # imgs_validate = np.load('imgs_validate.npy')\n",
        "\n",
        "def rescale_data():\n",
        "  scaler = preprocessing.MinMaxScaler()\n",
        "  mfcc = []\n",
        "  i = 0\n",
        "  for i, (d) in enumerate(data['mfcc']):\n",
        "    scaler.fit(d)\n",
        "    transform = scaler.transform(d)\n",
        "    mfcc.append(transform)\n",
        "  x = np.array(mfcc)\n",
        "  y = np.array(data['labels'])\n",
        "\n",
        "  return x, y\n",
        "\n",
        "def load_auto(path):\n",
        "  with open(os.path.join(BASE, DATA_SAVE_PATH, path), 'r') as fp:\n",
        "    data = json.load(fp)\n",
        "  x = np.empty(shape=(len(data['mfcc']), 18))\n",
        "  y = np.empty(shape=(len(data['labels']), 1))\n",
        "  for i in data['mfcc']:\n",
        "    np.append(x, np.array(i))\n",
        "\n",
        "  for i in data['lables']:\n",
        "    np.append(y, )\n",
        "\n",
        "  x = np.array()\n",
        "  y = np.array()\n",
        "\n",
        "\n",
        "# path to images dataset. Recommended to write down entire path to avoid errors (i.e \"C:/User/Desktop.../raw/)\n",
        "def load_data(path):\n",
        "  with open(os.path.join(BASE, DATA_SAVE_PATH, path), 'r') as fp:\n",
        "    data = json.load(fp)\n",
        "\n",
        "  x = np.array(data['mfcc'])\n",
        "  y = np.array(data['labels'])\n",
        "  return x, y\n",
        "\n",
        "def load_splits():\n",
        "  with open(os.path.join(BASE, DATA_SAVE_PATH, '10-fold-splits.json'), 'r') as fp:\n",
        "    data = json.load(fp)\n",
        "\n",
        "  return data\n",
        "\n",
        "\n",
        "def prepare_datasets(path, test_size, validation_size=None, cnn=False):\n",
        "\n",
        "  data = {}\n",
        "  X, y = load_npy(path)\n",
        "  # create train/test split\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
        "\n",
        "  if validation_size:\n",
        "    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validation_size)\n",
        "\n",
        "  if cnn:\n",
        "    X_train = X_train[..., np.newaxis]\n",
        "    X_test = X_test[..., np.newaxis]\n",
        "    if validation_size:\n",
        "      X_validation = X_validation[..., np.newaxis]\n",
        "\n",
        "  data['X_train'] = X_train\n",
        "  data['X_test'] = X_test\n",
        "  data['y_train'] = y_train\n",
        "  data['y_test'] = y_test\n",
        "\n",
        "  if validation_size:\n",
        "    data['X_validation'] = X_validation\n",
        "    data['y_validation'] = y_validation\n",
        "\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUXsvC8ht5cE"
      },
      "source": [
        "def build_cnn(input_shape):\n",
        "\n",
        "  model = keras.Sequential()\n",
        "\n",
        "  #1st conv layer\n",
        "  model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "  model.add(keras.layers.MaxPool2D((3, 3), strides=(2, 2), padding='same'))\n",
        "  model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "  # #2nd conv layer\n",
        "  model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "  model.add(keras.layers.MaxPool2D((3, 3), strides=(2, 2), padding='same'))\n",
        "  model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "  # #3rd conv layer\n",
        "  model.add(keras.layers.Conv2D(32, (2, 2), activation='relu', input_shape=input_shape))\n",
        "  model.add(keras.layers.MaxPool2D((2, 2), strides=(2, 2), padding='same'))\n",
        "  model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "  # # flatten the output and feed it into the dense layer\n",
        "  model.add(keras.layers.Flatten()) # converts 2D data into 1D\n",
        "  # TODO 1024 add one more dense layer. Smoothly reduce # of neurons\n",
        "  # Include new features here\n",
        "\n",
        "  model.add(keras.layers.Dense(64, activation='relu'))\n",
        "  model.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "  # # output layer\n",
        "  model.add(keras.layers.Dense(2, activation='softmax'))\n",
        "\n",
        "  return model\n",
        "\n",
        "def build_cnn_v2(input_shape):\n",
        "\n",
        "  model = keras.Sequential()\n",
        "\n",
        "  #1st conv layer\n",
        "  # TODO adjust using horizontal filter for temporal features\n",
        "  model.add(keras.layers.Conv2D(32, (2, 3), activation='relu', input_shape=input_shape))\n",
        "  model.add(keras.layers.MaxPool2D((4, 1), strides=(2, 2), padding='same'))\n",
        "  model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "  # #2nd conv layer\n",
        "  # TODO adjust using vertical filter for timbral features\n",
        "  model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "  model.add(keras.layers.MaxPool2D((2, 2), strides=(2, 2), padding='same'))\n",
        "  model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "  # #3rd conv layer\n",
        "  # TODO add 4th convolutional layer \n",
        "  model.add(keras.layers.Conv2D(32, (2, 2), activation='relu', input_shape=input_shape))\n",
        "  model.add(keras.layers.MaxPool2D((2, 2), strides=(2, 2), padding='same'))\n",
        "  model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "  # # flatten the output and feed it into the dense layer\n",
        "  model.add(keras.layers.Flatten()) # converts 2D data into 1D\n",
        "  # TODO 1024 add one more dense layer. Smoothly reduce # of neurons\n",
        "  # Include new features here\n",
        "  model.add(keras.layers.Dense(1024, activation='relu'))\n",
        "  # model.add(keras.layers.Dense(512, activation='relu'))\n",
        "  model.add(keras.layers.Dense(64, activation='relu'))\n",
        "  model.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "  # # output layer\n",
        "  model.add(keras.layers.Dense(2, activation='softmax'))\n",
        "\n",
        "  return model\n",
        "\n",
        "def build_cnn_v3(input_shape):\n",
        "\n",
        "  # Adapted from Keunwoo Choi \n",
        "  model = keras.Sequential()\n",
        "\n",
        "  #1st conv layer\n",
        "  # TODO adjust using horizontal filter for temporal features\n",
        "  model.add(keras.layers.Conv2D(128, (3, 3), activation='relu', input_shape=input_shape))\n",
        "  model.add(keras.layers.MaxPool2D((2, 4), padding='same'))\n",
        "  model.add(keras.layers.Dropout(0.5))\n",
        "\n",
        "  model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "  # #2nd conv layer\n",
        "  # #TODO adjust using vertical filter for timbral features\n",
        "  model.add(keras.layers.Conv2D(256, (3, 3), activation='relu', input_shape=input_shape))\n",
        "  model.add(keras.layers.MaxPool2D((2, 4), padding='same'))\n",
        "  model.add(keras.layers.Dropout(0.5))\n",
        "\n",
        "  model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "  model.add(keras.layers.Conv2D(512, (3, 3), activation='relu', input_shape=input_shape))\n",
        "  model.add(keras.layers.MaxPool2D((2, 4), padding='same'))\n",
        "  model.add(keras.layers.Dropout(0.5))\n",
        "\n",
        "  model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "  model.add(keras.layers.Conv2D(1024, (3, 3), activation='relu', input_shape=input_shape))\n",
        "  model.add(keras.layers.MaxPool2D((3, 5), padding='same'))\n",
        "  model.add(keras.layers.Dropout(0.5))\n",
        "\n",
        "  model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "  model.add(keras.layers.Conv2D(2048, (3, 3), activation='relu', input_shape=input_shape))\n",
        "  model.add(keras.layers.MaxPool2D((4, 8), padding='same'))\n",
        "  model.add(keras.layers.Dropout(0.5))\n",
        "  model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "  model.add(keras.layers.Conv2D(1024, (1, 1), activation='relu', padding='same', input_shape=input_shape))\n",
        "  # # # #3rd conv layer\n",
        "  # # # TODO add 4th convolutional layer \n",
        "  # model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "  # model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "  # model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "  # model.add(keras.layers.MaxPool2D((2, 2), padding='same'))\n",
        "  # model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "  # # flatten the output and feed it into the dense layer\n",
        "  # model.add(keras.layers.Flatten()) # converts 2D data into 1D\n",
        "  # TODO 1024 add one more dense layer. Smoothly reduce # of neurons\n",
        "  # Include new features here\n",
        "  # model.add(keras.layers.Dense(1024, activation='relu'))\n",
        "  # model.add(keras.layers.Dense(512, activation='relu'))\n",
        "  # model.add(keras.layers.Dense(64, activation='relu'))\n",
        "  # model.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "  # # output layer\n",
        "  model.add(keras.layers.Dense(2, activation='sigmoid'))\n",
        "\n",
        "  return model\n",
        "\n",
        "def build_cnn_v4(input_shape):\n",
        "  # End-to-end learning Sander Dieleman, Benjamin Schrauwen\n",
        "  model = keras.Sequential()\n",
        "\n",
        "  #1st conv layer\n",
        "  # TODO adjust using horizontal filter for temporal features\n",
        "  model.add(keras.layers.Conv2D(32, (1, 8), activation='relu', input_shape=input_shape))\n",
        "  model.add(keras.layers.MaxPool2D((1, 4), padding='same'))\n",
        "\n",
        "  model.add(keras.layers.Conv2D(32, (1, 8), activation='relu', input_shape=input_shape))\n",
        "  model.add(keras.layers.MaxPool2D((1, 4), padding='same'))\n",
        "\n",
        "  # # flatten the output and feed it into the dense layer\n",
        "  model.add(keras.layers.Flatten()) # converts 2D data into 1D\n",
        "  # TODO 1024 add one more dense layer. Smoothly reduce # of neurons\n",
        "  # Include new features here\n",
        "  model.add(keras.layers.Dense(100, activation='relu'))\n",
        "  # model.add(keras.layers.Dense(512, activation='relu'))\n",
        "  model.add(keras.layers.Dense(50, activation='relu'))\n",
        "  model.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "  # # output layer\n",
        "  model.add(keras.layers.Dense(2, activation='sigmoid'))\n",
        "\n",
        "  return model\n",
        "\n",
        "def build_cnn_v5(input_shape):\n",
        "\n",
        "  # Adapted from Jordi Pons\n",
        "\n",
        "  inputs = keras.Input(shape=input_shape)\n",
        "  print(inputs.shape)\n",
        "\n",
        "  # Block One\n",
        "  one_x_100 = keras.layers.Conv2D(10, (100, 1), activation='elu')(inputs)\n",
        "  mp_one_x_100 = keras.layers.MaxPool2D((one_x_100.shape[1], 4))(one_x_100)\n",
        "  # mp_one_x_100 = keras.layers.ZeroPadding2D(padding=((1, 0), (0, 0)))(mp_one_x_100)\n",
        "\n",
        "  three_x_100 = keras.layers.Conv2D(6, (100, 3), strides=(1, 1), activation='elu')(inputs)\n",
        "  # three_x_100 = keras.layers.ZeroPadding2D(padding=1)(three_x_100)\n",
        "  mp_three_x_100 = keras.layers.MaxPool2D((three_x_100.shape[1],4), padding='same')(three_x_100)\n",
        "  # mp_three_x_100 = keras.layers.ZeroPadding2D(padding=((1, 0), (0, 0)))(mp_three_x_100)\n",
        "\n",
        "  five_x_100 = keras.layers.ZeroPadding2D(padding=2)(inputs)\n",
        "  five_x_100 = keras.layers.Conv2D(3, (100, 5), strides=(1, 1), activation='elu')(five_x_100)\n",
        "  mp_five_x_100 = keras.layers.MaxPool2D((five_x_100.shape[1], 4), padding='same')(five_x_100)\n",
        "\n",
        "  seven_x_100 = keras.layers.ZeroPadding2D(padding=3)(inputs)\n",
        "  seven_x_100 = keras.layers.Conv2D(3, (100, 7), strides=(1, 1), activation='elu')(seven_x_100)\n",
        "  mp_seven_x_100 = keras.layers.MaxPool2D((seven_x_100.shape[1], 4), padding='same')(seven_x_100)\n",
        "\n",
        "\n",
        "  # Block Two\n",
        "  one_x_75 = keras.layers.Conv2D(15, (100, 1), activation='elu')(inputs)\n",
        "  mp_one_x_75 = keras.layers.MaxPool2D((one_x_75.shape[1], 4), padding='same')(one_x_75)\n",
        "  # mp_one_x_75 = keras.layers.ZeroPadding2D(padding=((1, 0), (0, 0)))(mp_one_x_75)\n",
        "\n",
        "  three_x_75 = keras.layers.Conv2D(10, (100, 3), strides=(1, 1), activation='elu')(inputs)\n",
        "  # three_x_75 = keras.layers.ZeroPadding2D(padding=1)(three_x_75)\n",
        "  mp_three_x_75 = keras.layers.MaxPool2D((three_x_75.shape[1], 4), padding='same')(three_x_75)\n",
        "  # mp_three_x_75 = keras.layers.ZeroPadding2D(padding=((1, 0), (0, 0)))(mp_three_x_75)\n",
        "\n",
        "  five_x_75 = keras.layers.ZeroPadding2D(padding=2)(inputs)\n",
        "  five_x_75 = keras.layers.Conv2D(5, (100, 5), strides=(1, 1), activation='elu')(five_x_75)\n",
        "  mp_five_x_75 = keras.layers.MaxPool2D((five_x_75.shape[1], 4), padding='same')(five_x_75)\n",
        "\n",
        "  seven_x_75 = keras.layers.ZeroPadding2D(padding=3)(inputs)\n",
        "  seven_x_75 = keras.layers.Conv2D(5, (100, 7), strides=(1, 1), activation='elu')(seven_x_75)\n",
        "  mp_seven_x_75 = keras.layers.MaxPool2D((seven_x_75.shape[1], 4), padding='same')(seven_x_75)\n",
        "\n",
        "  # Block 3\n",
        "  one_x_25 = keras.layers.Conv2D(15, (100, 1), activation='elu')(inputs)\n",
        "  mp_one_x_25 = keras.layers.MaxPool2D((one_x_25.shape[1], 4), padding='same')(one_x_25)\n",
        "  # mp_one_x_25 = keras.layers.ZeroPadding2D(padding=((1, 0), (0, 0)))(mp_one_x_25)\n",
        "\n",
        "  three_x_25 = keras.layers.ZeroPadding2D(padding=1)(inputs)\n",
        "  three_x_25 = keras.layers.Conv2D(10, (100, 3), strides=(1, 1), activation='elu')(three_x_25)\n",
        "  mp_three_x_25 = keras.layers.MaxPool2D((three_x_25.shape[1], 4), padding='same')(three_x_25)\n",
        "  # mp_three_x_25 = keras.layers.ZeroPadding2D(padding=((1, 0), (0, 0)))(mp_three_x_25)\n",
        "\n",
        "  five_x_25 = keras.layers.Conv2D(5, (100, 5), strides=(1, 1), activation='elu')(inputs)\n",
        "  five_x_25 = keras.layers.ZeroPadding2D(padding=2)(five_x_25)\n",
        "  mp_five_x_25 = keras.layers.MaxPool2D((five_x_25.shape[1], 4), padding='same')(five_x_25)\n",
        "\n",
        "  seven_x_25 = keras.layers.Conv2D(5, (100, 7), strides=(1, 1), activation='elu')(inputs)\n",
        "  seven_x_25 = keras.layers.ZeroPadding2D(padding=3)(seven_x_25)\n",
        "  mp_seven_x_25 = keras.layers.MaxPool2D((seven_x_25.shape[1], 4), padding='same')(seven_x_25)\n",
        "\n",
        "  # print(\"Block 1\")\n",
        "  # print(mp_one_x_100.shape)\n",
        "  # print(mp_three_x_100.shape)\n",
        "  # print(mp_five_x_100.shape)\n",
        "  # print(mp_seven_x_100.shape)\n",
        "\n",
        "  # print(\"Block 2\")\n",
        "  # print(mp_one_x_75.shape)\n",
        "  # print(mp_three_x_75.shape)\n",
        "  # print(mp_five_x_75.shape)\n",
        "  # print(mp_seven_x_75.shape)\n",
        "\n",
        "  # print(\"Block 3\")\n",
        "  # print(mp_one_x_25.shape)\n",
        "  # print(mp_three_x_25.shape)\n",
        "  # print(mp_five_x_25.shape)\n",
        "  # print(mp_seven_x_25.shape)\n",
        "\n",
        "  music_filters = keras.layers.Concatenate(axis=-1)([mp_one_x_100, mp_three_x_100, mp_five_x_100, mp_seven_x_100,\n",
        "                                                     mp_one_x_75, mp_three_x_75, mp_five_x_75, mp_seven_x_75,\n",
        "                                                     mp_one_x_25, mp_three_x_25, mp_five_x_25, mp_seven_x_25])\n",
        "  print(music_filters.shape)  # (None, 1, 323, 92)\n",
        "  reshape = keras.layers.Reshape((music_filters.shape[3], music_filters.shape[2], music_filters.shape[1]))(music_filters)\n",
        "\n",
        "  print('\\n Reshape')\n",
        "  print(reshape.shape) # (None, 92, 323, 1)\n",
        "\n",
        "  layer_4 = keras.layers.Conv2D(32, (reshape.shape[1], 8), activation='elu')(reshape)\n",
        "  \n",
        "  print('\\n Four')\n",
        "  print(layer_4.shape)\n",
        "\n",
        "  five_pre = keras.layers.MaxPool2D((1, 4), padding='same')(layer_4)\n",
        "  five = keras.layers.Reshape((five_pre.shape[3], five_pre.shape[2], five_pre.shape[1]))(five_pre)\n",
        "\n",
        "  print('\\n Five')\n",
        "  print(five_pre.shape)\n",
        "  print(five.shape)\n",
        "\n",
        "  # Dense Layers\n",
        "  flat = keras.layers.Flatten()(five)\n",
        "  dropout1 = keras.layers.Dropout(0.5)(flat)\n",
        "  six = keras.layers.Dense(100, activation='relu')(dropout1)\n",
        "  dropout2 = keras.layers.Dropout(0.5)(six)\n",
        "  seven = keras.layers.Dense(2, activation='sigmoid')(dropout2)\n",
        "  model = Model(inputs, [seven])\n",
        "\n",
        "  return model\n",
        "\n",
        "def build_lstm(input_shape):\n",
        "\n",
        "  model = keras.Sequential()\n",
        "\n",
        "  model.add(keras.layers.LSTM(256, input_shape=input_shape, return_sequences=True))\n",
        "  model.add(keras.layers.LSTM(256))\n",
        "\n",
        "  model.add(keras.layers.Dense(256, activation='relu'))\n",
        "  model.add(keras.layers.Dense(256, activation='relu'))\n",
        "  model.add(keras.layers.Dense(256, activation='relu'))\n",
        "  model.add(keras.layers.Dense(256, activation='relu'))\n",
        "\n",
        "  model.add(keras.layers.Dropout(0.30))\n",
        "\n",
        "  model.add(keras.layers.Dense(2, activation='softmax'))\n",
        "\n",
        "  return model\n",
        "\n",
        "def plot_history(history):\n",
        "    \"\"\"Plots accuracy/loss for training/validation set as a function of the epochs\n",
        "        :param history: Training history of model\n",
        "        :return:\n",
        "    \"\"\"\n",
        "\n",
        "    fig, axs = plt.subplots(2, figsize=(5, 5))\n",
        "\n",
        "    # create accuracy sublpot\n",
        "    axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\n",
        "    axs[0].plot(history.history[\"val_accuracy\"], label=\"val accuracy\")\n",
        "    axs[0].set_ylabel(\"Accuracy\")\n",
        "    axs[0].legend(loc=\"upper left\")\n",
        "    axs[0].set_title(\"Accuracy eval\")\n",
        "\n",
        "    # create error sublpot\n",
        "    axs[1].plot(history.history[\"loss\"], label=\"train error\")\n",
        "    axs[1].plot(history.history[\"val_loss\"], label=\"val error\")\n",
        "    axs[1].set_ylabel(\"Error\")\n",
        "    axs[1].set_xlabel(\"Epoch\")\n",
        "    axs[1].legend(loc=\"upper left\")\n",
        "    axs[1].set_title(\"Error eval\")\n",
        "    # plt.figure(figsize=(30,30))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnpL5sLkiGz5"
      },
      "source": [
        "def run_cnn(X_train, X_test, y_train, y_test):\n",
        "  print(X_train.shape[1])\n",
        "  print(X_train.shape[2])\n",
        "  print(X_train.shape[3])\n",
        "\n",
        "  input_shape = (X_train.shape[1], X_train.shape[2], X_train.shape[3])\n",
        "  model = build_cnn_v5(input_shape)\n",
        "  optimizer = keras.optimizers.Adam(learning_rate=1e-5)\n",
        "  model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
        "  weights = os.path.join(BASE, 'Thesis', 'weights.h5')\n",
        "  callbacks = [keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=100, min_delta=0, mode='max'), keras.callbacks.ModelCheckpoint(weights, monitor='val_accuracy', mode='max', save_best_only=True)]\n",
        "\n",
        "  history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=16, epochs=300, callbacks=callbacks)\n",
        "  m = max(history.history['val_accuracy'])\n",
        "  plot_history(history)\n",
        "  model.load_weights(weights)\n",
        "  return model, m\n",
        "\n",
        "def run_lstm(X_train, X_test, y_train, y_test):\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "    model = build_model(input_shape)\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=64, epochs=30)\n",
        "    plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDDKNMES8Tdj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "146edc66-d076-42c7-bab3-ed3dc5f73a47"
      },
      "source": [
        "model = build_cnn_v5((128, 1292, 1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 128, 1292, 1)\n",
            "(None, 1, 323, 92)\n",
            "\n",
            " Reshape\n",
            "(None, 92, 323, 1)\n",
            "\n",
            " Four\n",
            "(None, 1, 316, 32)\n",
            "\n",
            " Five\n",
            "(None, 1, 79, 32)\n",
            "(None, 32, 79, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXcI0ZKN8q7o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46d0325a-c200-4791-8544-f1d219eda698"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 128, 1292, 1 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d (ZeroPadding2D)  (None, 132, 1296, 1) 0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_1 (ZeroPadding2D (None, 134, 1298, 1) 0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_2 (ZeroPadding2D (None, 132, 1296, 1) 0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_3 (ZeroPadding2D (None, 134, 1298, 1) 0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_4 (ZeroPadding2D (None, 130, 1294, 1) 0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 29, 1288, 5)  2505        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 29, 1286, 5)  3505        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 29, 1292, 10) 1010        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 29, 1290, 6)  1806        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 33, 1292, 3)  1503        zero_padding2d[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 35, 1292, 3)  2103        zero_padding2d_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 29, 1292, 15) 1515        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 29, 1290, 10) 3010        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 33, 1292, 5)  2505        zero_padding2d_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 35, 1292, 5)  3505        zero_padding2d_3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 29, 1292, 15) 1515        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 31, 1292, 10) 3010        zero_padding2d_4[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_5 (ZeroPadding2D (None, 33, 1292, 5)  0           conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_6 (ZeroPadding2D (None, 35, 1292, 5)  0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 1, 323, 10)   0           conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 1, 323, 6)    0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 1, 323, 3)    0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 1, 323, 3)    0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 1, 323, 15)   0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 1, 323, 10)   0           conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 1, 323, 5)    0           conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2D)  (None, 1, 323, 5)    0           conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2D)  (None, 1, 323, 15)   0           conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2D)  (None, 1, 323, 10)   0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling2D) (None, 1, 323, 5)    0           zero_padding2d_5[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling2D) (None, 1, 323, 5)    0           zero_padding2d_6[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 1, 323, 92)   0           max_pooling2d[0][0]              \n",
            "                                                                 max_pooling2d_1[0][0]            \n",
            "                                                                 max_pooling2d_2[0][0]            \n",
            "                                                                 max_pooling2d_3[0][0]            \n",
            "                                                                 max_pooling2d_4[0][0]            \n",
            "                                                                 max_pooling2d_5[0][0]            \n",
            "                                                                 max_pooling2d_6[0][0]            \n",
            "                                                                 max_pooling2d_7[0][0]            \n",
            "                                                                 max_pooling2d_8[0][0]            \n",
            "                                                                 max_pooling2d_9[0][0]            \n",
            "                                                                 max_pooling2d_10[0][0]           \n",
            "                                                                 max_pooling2d_11[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 92, 323, 1)   0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 1, 316, 32)   23584       reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling2D) (None, 1, 79, 32)    0           conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 32, 79, 1)    0           max_pooling2d_12[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 2528)         0           reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 2528)         0           flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 100)          252900      dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 100)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            202         dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 304,178\n",
            "Trainable params: 304,178\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VA4ZMD73PYhA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "24ea6d86-c753-49d2-bbb9-ba0904304081"
      },
      "source": [
        "splits = load_splits()\n",
        "maxes = []\n",
        "ensembleMax = []\n",
        "for fold in range(3, 10):\n",
        "\n",
        "  testSamples = splits[fold]['test']\n",
        "  trainLength = 0\n",
        "  testLength = 0\n",
        "  ytrain = []\n",
        "  ytest = []\n",
        "  songSections = []\n",
        "  songClassification = []\n",
        "\n",
        "\n",
        "  # Calculates the size of train and test folds\n",
        "  for i, (file) in enumerate(splits[fold]['train']):\n",
        "    dir = os.path.join(BASE, DATA_SAVE_PATH, 'Spectrograms_30sec', file.replace(\".mp3\", \"*\"))\n",
        "    clipSections = len(glob.glob(dir))\n",
        "    trainLength+= clipSections\n",
        "    if os.path.exists(os.path.join(BASE, 'Thesis', 'Samples', 'FULL_TRACKS', 'noDJ', file)):\n",
        "      ytrain_ = [0]*clipSections\n",
        "    elif os.path.exists(os.path.join(BASE, 'Thesis', 'Samples', 'FULL_TRACKS', 'yesDJ', file)):\n",
        "      ytrain_ = [1]*clipSections\n",
        "    else:\n",
        "      print(\"Could not find file\", file)\n",
        "    ytrain = ytrain+ytrain_\n",
        "  \n",
        "  for i, (file) in enumerate(splits[fold]['test']):\n",
        "    dir = os.path.join(BASE, DATA_SAVE_PATH, 'Spectrograms_30sec', file.replace(\".mp3\", \"*\"))\n",
        "    clipSections = len(glob.glob(dir))\n",
        "    testLength+= clipSections\n",
        "    songSections.append(clipSections)\n",
        "    if os.path.exists(os.path.join(BASE, 'Thesis', 'Samples', 'FULL_TRACKS', 'noDJ', file)):\n",
        "      ytest_ = [0]*clipSections\n",
        "      songClassification.append(0)\n",
        "    elif os.path.exists(os.path.join(BASE, 'Thesis', 'Samples', 'FULL_TRACKS', 'yesDJ', file)):\n",
        "      ytest_ = [1]*clipSections\n",
        "      songClassification.append(1)\n",
        "    else:\n",
        "      print(\"Could not find file\", file)\n",
        "\n",
        "    ytest = ytest+ytest_\n",
        "  \n",
        "  # Create test/train data arrays\n",
        "\n",
        "  x_train = np.ndarray((trainLength, 128, 1292), dtype=np.uint8)\n",
        "  x_test = np.ndarray((testLength, 128, 1292), dtype=np.uint8)\n",
        "  y_train = np.array(ytrain)\n",
        "  y_test = np.array(ytest)\n",
        "\n",
        "  # Load test/train data\n",
        "  loc = 0\n",
        "  for url in splits[fold]['train']:\n",
        "    url = url.replace('.mp3', '')\n",
        "    for loadpath in glob.glob(os.path.join(BASE, DATA_SAVE_PATH, 'Spectrograms_30sec', url+'*.npy')):\n",
        "      x_train[loc] = np.load(loadpath)\n",
        "      loc+=1\n",
        "  x_train = x_train.astype('float32')\n",
        "  x_train = x_train.astype('float32')\n",
        "  mean = np.mean(x_train)\n",
        "  std = np.mean(x_train)\n",
        "  x_train -= mean\n",
        "  x_train /= std\n",
        "  x_train = x_train[..., np.newaxis]\n",
        "\n",
        "  loc = 0\n",
        "  for url in splits[fold]['test']:\n",
        "    url = url.replace('.mp3', '')\n",
        "    for loadpath in glob.glob(os.path.join(BASE, DATA_SAVE_PATH, 'Spectrograms_30sec', url+'*.npy')):\n",
        "      x_test[loc] = np.load(loadpath)\n",
        "      loc+=1\n",
        "  x_test = x_test.astype('float32')\n",
        "  x_test = x_test.astype('float32')\n",
        "  mean = np.mean(x_test)\n",
        "  std = np.mean(x_test)\n",
        "  x_test -= mean\n",
        "  x_test /= std\n",
        "  x_test = x_test[..., np.newaxis]\n",
        "\n",
        "  print(f\"Starting {fold}\")\n",
        "  model, m = run_cnn(x_train, x_test, y_train, y_test)\n",
        "\n",
        "  # Ensemble voting\n",
        "  start = 0\n",
        "  print(model.predict(x_test))\n",
        "  predictions = []\n",
        "  # sig_predictions = model.predict(x_test)\n",
        "  # for i in sig_predictions:\n",
        "  #   predictions.append(np.argmax(i[0][0]))\n",
        "  predictions = np.argmax(model.predict(x_test), axis=1)\n",
        "\n",
        "  ensemblePrediction = []\n",
        "  for song in range(0, len(songClassification)):\n",
        "    correct = 0\n",
        "    incorrect = 0\n",
        "    # print(splits[fold]['test'][song])\n",
        "    print(y_test[start:start+songSections[song]])\n",
        "    print(predictions[start:start+songSections[song]])\n",
        "    for i, j in zip(y_test[start:start+songSections[song]], predictions[start:start+songSections[song]]):\n",
        "      if i==j==songClassification[song]:\n",
        "        correct+=1\n",
        "    # print(f\"{correct} {songSections[song]}\")\n",
        "    start=start+songSections[song]\n",
        "    ensemblePrediction.append(correct/songSections[song])\n",
        "\n",
        "\n",
        "  correctPreds = 0\n",
        "  incorrectPreds = 0\n",
        "  for i in ensemblePrediction:\n",
        "    if i >= .50:\n",
        "      correctPreds+=1\n",
        "    else:\n",
        "      incorrectPreds+=1\n",
        "    \n",
        "  maxes.append(m)\n",
        "  ensembleMax.append(correctPreds/(correctPreds+incorrectPreds))\n",
        "  \n",
        "  print(f\"Correct Predictions: {correctPreds}, incorrect predictions: {incorrectPreds}\")\n",
        "\n",
        "  # maxes.append(m)\n",
        "# avg = sum(maxes)/len(maxes)\n",
        "# std = statistics.stdev(maxes)\n",
        "# print(f\"AVG: {avg} STD: {std} \")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting 2\n",
            "128\n",
            "1292\n",
            "1\n",
            "(None, 128, 1292, 1)\n",
            "(None, 1, 323, 92)\n",
            "\n",
            " Reshape\n",
            "(None, 92, 323, 1)\n",
            "\n",
            " Four\n",
            "(None, 1, 316, 32)\n",
            "\n",
            " Five\n",
            "(None, 1, 79, 32)\n",
            "(None, 32, 79, 1)\n",
            "Epoch 1/300\n",
            "143/143 [==============================] - 9s 60ms/step - loss: 0.6913 - accuracy: 0.5295 - val_loss: 0.6991 - val_accuracy: 0.4494\n",
            "Epoch 2/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.6890 - accuracy: 0.5650 - val_loss: 0.7012 - val_accuracy: 0.4494\n",
            "Epoch 3/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.6870 - accuracy: 0.5637 - val_loss: 0.7035 - val_accuracy: 0.4494\n",
            "Epoch 4/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.6861 - accuracy: 0.5606 - val_loss: 0.7048 - val_accuracy: 0.4494\n",
            "Epoch 5/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.6867 - accuracy: 0.5571 - val_loss: 0.7039 - val_accuracy: 0.4494\n",
            "Epoch 6/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.6881 - accuracy: 0.5536 - val_loss: 0.7043 - val_accuracy: 0.4494\n",
            "Epoch 7/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.6852 - accuracy: 0.5593 - val_loss: 0.7042 - val_accuracy: 0.4494\n",
            "Epoch 8/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.6848 - accuracy: 0.5606 - val_loss: 0.7058 - val_accuracy: 0.4494\n",
            "Epoch 9/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.6854 - accuracy: 0.5602 - val_loss: 0.7033 - val_accuracy: 0.4494\n",
            "Epoch 10/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.6859 - accuracy: 0.5593 - val_loss: 0.7030 - val_accuracy: 0.4494\n",
            "Epoch 11/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.6833 - accuracy: 0.5602 - val_loss: 0.7047 - val_accuracy: 0.4494\n",
            "Epoch 12/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.6834 - accuracy: 0.5632 - val_loss: 0.7031 - val_accuracy: 0.4494\n",
            "Epoch 13/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.6824 - accuracy: 0.5654 - val_loss: 0.7013 - val_accuracy: 0.4494\n",
            "Epoch 14/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.6806 - accuracy: 0.5667 - val_loss: 0.7048 - val_accuracy: 0.4494\n",
            "Epoch 15/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.6811 - accuracy: 0.5654 - val_loss: 0.7007 - val_accuracy: 0.4494\n",
            "Epoch 16/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.6805 - accuracy: 0.5694 - val_loss: 0.6984 - val_accuracy: 0.4494\n",
            "Epoch 17/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.6796 - accuracy: 0.5689 - val_loss: 0.7009 - val_accuracy: 0.4494\n",
            "Epoch 18/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.6780 - accuracy: 0.5676 - val_loss: 0.6986 - val_accuracy: 0.4494\n",
            "Epoch 19/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.6775 - accuracy: 0.5641 - val_loss: 0.6999 - val_accuracy: 0.4494\n",
            "Epoch 20/300\n",
            "143/143 [==============================] - 8s 57ms/step - loss: 0.6752 - accuracy: 0.5755 - val_loss: 0.6934 - val_accuracy: 0.4534\n",
            "Epoch 21/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.6740 - accuracy: 0.5772 - val_loss: 0.6926 - val_accuracy: 0.4534\n",
            "Epoch 22/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.6710 - accuracy: 0.5912 - val_loss: 0.6964 - val_accuracy: 0.4534\n",
            "Epoch 23/300\n",
            "143/143 [==============================] - 8s 57ms/step - loss: 0.6679 - accuracy: 0.5821 - val_loss: 0.6911 - val_accuracy: 0.4575\n",
            "Epoch 24/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.6662 - accuracy: 0.5969 - val_loss: 0.6931 - val_accuracy: 0.4575\n",
            "Epoch 25/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.6683 - accuracy: 0.5965 - val_loss: 0.6898 - val_accuracy: 0.4534\n",
            "Epoch 26/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.6607 - accuracy: 0.6079 - val_loss: 0.6896 - val_accuracy: 0.4534\n",
            "Epoch 27/300\n",
            "143/143 [==============================] - 8s 58ms/step - loss: 0.6582 - accuracy: 0.6031 - val_loss: 0.6818 - val_accuracy: 0.4818\n",
            "Epoch 28/300\n",
            "143/143 [==============================] - 8s 55ms/step - loss: 0.6504 - accuracy: 0.6214 - val_loss: 0.6719 - val_accuracy: 0.5749\n",
            "Epoch 29/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.6539 - accuracy: 0.6083 - val_loss: 0.6712 - val_accuracy: 0.5709\n",
            "Epoch 30/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.6490 - accuracy: 0.6333 - val_loss: 0.6683 - val_accuracy: 0.5749\n",
            "Epoch 31/300\n",
            "143/143 [==============================] - 8s 58ms/step - loss: 0.6423 - accuracy: 0.6368 - val_loss: 0.6588 - val_accuracy: 0.6154\n",
            "Epoch 32/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.6379 - accuracy: 0.6420 - val_loss: 0.6618 - val_accuracy: 0.5749\n",
            "Epoch 33/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.6393 - accuracy: 0.6446 - val_loss: 0.6612 - val_accuracy: 0.5668\n",
            "Epoch 34/300\n",
            "143/143 [==============================] - 8s 57ms/step - loss: 0.6315 - accuracy: 0.6578 - val_loss: 0.6456 - val_accuracy: 0.6356\n",
            "Epoch 35/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.6290 - accuracy: 0.6661 - val_loss: 0.6513 - val_accuracy: 0.5789\n",
            "Epoch 36/300\n",
            "143/143 [==============================] - 8s 57ms/step - loss: 0.6264 - accuracy: 0.6600 - val_loss: 0.6426 - val_accuracy: 0.6518\n",
            "Epoch 37/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.6207 - accuracy: 0.6810 - val_loss: 0.6438 - val_accuracy: 0.5992\n",
            "Epoch 38/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.6090 - accuracy: 0.6888 - val_loss: 0.6438 - val_accuracy: 0.5870\n",
            "Epoch 39/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.6074 - accuracy: 0.6928 - val_loss: 0.6315 - val_accuracy: 0.6316\n",
            "Epoch 40/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.6005 - accuracy: 0.6945 - val_loss: 0.6410 - val_accuracy: 0.5830\n",
            "Epoch 41/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.5970 - accuracy: 0.6963 - val_loss: 0.6212 - val_accuracy: 0.6397\n",
            "Epoch 42/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.5945 - accuracy: 0.6932 - val_loss: 0.6134 - val_accuracy: 0.6518\n",
            "Epoch 43/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.5916 - accuracy: 0.7024 - val_loss: 0.6212 - val_accuracy: 0.6275\n",
            "Epoch 44/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.5745 - accuracy: 0.7212 - val_loss: 0.6158 - val_accuracy: 0.6275\n",
            "Epoch 45/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.5821 - accuracy: 0.6976 - val_loss: 0.6077 - val_accuracy: 0.6478\n",
            "Epoch 46/300\n",
            "143/143 [==============================] - 8s 57ms/step - loss: 0.5650 - accuracy: 0.7239 - val_loss: 0.5987 - val_accuracy: 0.6559\n",
            "Epoch 47/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.5645 - accuracy: 0.7168 - val_loss: 0.5881 - val_accuracy: 0.6883\n",
            "Epoch 48/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.5636 - accuracy: 0.7243 - val_loss: 0.5882 - val_accuracy: 0.6640\n",
            "Epoch 49/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.5580 - accuracy: 0.7269 - val_loss: 0.5902 - val_accuracy: 0.6478\n",
            "Epoch 50/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.5475 - accuracy: 0.7427 - val_loss: 0.5733 - val_accuracy: 0.6842\n",
            "Epoch 51/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.5362 - accuracy: 0.7492 - val_loss: 0.5798 - val_accuracy: 0.6721\n",
            "Epoch 52/300\n",
            "143/143 [==============================] - 8s 57ms/step - loss: 0.5432 - accuracy: 0.7374 - val_loss: 0.5674 - val_accuracy: 0.6923\n",
            "Epoch 53/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.5294 - accuracy: 0.7558 - val_loss: 0.5648 - val_accuracy: 0.6842\n",
            "Epoch 54/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.5278 - accuracy: 0.7488 - val_loss: 0.5778 - val_accuracy: 0.6680\n",
            "Epoch 55/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.5231 - accuracy: 0.7558 - val_loss: 0.5634 - val_accuracy: 0.6842\n",
            "Epoch 56/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.5095 - accuracy: 0.7654 - val_loss: 0.5537 - val_accuracy: 0.6883\n",
            "Epoch 57/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.5128 - accuracy: 0.7571 - val_loss: 0.5682 - val_accuracy: 0.6599\n",
            "Epoch 58/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.5066 - accuracy: 0.7628 - val_loss: 0.5597 - val_accuracy: 0.6721\n",
            "Epoch 59/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.5076 - accuracy: 0.7663 - val_loss: 0.5508 - val_accuracy: 0.6721\n",
            "Epoch 60/300\n",
            "143/143 [==============================] - 8s 57ms/step - loss: 0.5003 - accuracy: 0.7751 - val_loss: 0.5424 - val_accuracy: 0.7004\n",
            "Epoch 61/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.4995 - accuracy: 0.7746 - val_loss: 0.5348 - val_accuracy: 0.7368\n",
            "Epoch 62/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.4836 - accuracy: 0.7834 - val_loss: 0.5352 - val_accuracy: 0.7045\n",
            "Epoch 63/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.4846 - accuracy: 0.7834 - val_loss: 0.5313 - val_accuracy: 0.7126\n",
            "Epoch 64/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.4824 - accuracy: 0.7812 - val_loss: 0.5303 - val_accuracy: 0.7126\n",
            "Epoch 65/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.4788 - accuracy: 0.7877 - val_loss: 0.5235 - val_accuracy: 0.7287\n",
            "Epoch 66/300\n",
            "143/143 [==============================] - 9s 63ms/step - loss: 0.4741 - accuracy: 0.7943 - val_loss: 0.5195 - val_accuracy: 0.7449\n",
            "Epoch 67/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.4717 - accuracy: 0.7943 - val_loss: 0.5243 - val_accuracy: 0.7126\n",
            "Epoch 68/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.4626 - accuracy: 0.7930 - val_loss: 0.5258 - val_accuracy: 0.7004\n",
            "Epoch 69/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.4691 - accuracy: 0.7899 - val_loss: 0.5303 - val_accuracy: 0.6883\n",
            "Epoch 70/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.4536 - accuracy: 0.8088 - val_loss: 0.5232 - val_accuracy: 0.7085\n",
            "Epoch 71/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.4517 - accuracy: 0.8057 - val_loss: 0.5210 - val_accuracy: 0.7166\n",
            "Epoch 72/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.4424 - accuracy: 0.8061 - val_loss: 0.5187 - val_accuracy: 0.7247\n",
            "Epoch 73/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.4370 - accuracy: 0.8144 - val_loss: 0.5273 - val_accuracy: 0.6802\n",
            "Epoch 74/300\n",
            "143/143 [==============================] - 8s 57ms/step - loss: 0.4446 - accuracy: 0.8009 - val_loss: 0.5079 - val_accuracy: 0.7530\n",
            "Epoch 75/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.4327 - accuracy: 0.8193 - val_loss: 0.5052 - val_accuracy: 0.7571\n",
            "Epoch 76/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.4285 - accuracy: 0.8175 - val_loss: 0.5114 - val_accuracy: 0.7206\n",
            "Epoch 77/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.4298 - accuracy: 0.8206 - val_loss: 0.5162 - val_accuracy: 0.7045\n",
            "Epoch 78/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.4275 - accuracy: 0.8144 - val_loss: 0.5174 - val_accuracy: 0.7045\n",
            "Epoch 79/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.4214 - accuracy: 0.8284 - val_loss: 0.5077 - val_accuracy: 0.7206\n",
            "Epoch 80/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.4141 - accuracy: 0.8232 - val_loss: 0.5066 - val_accuracy: 0.7166\n",
            "Epoch 81/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.4155 - accuracy: 0.8254 - val_loss: 0.5012 - val_accuracy: 0.7530\n",
            "Epoch 82/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.4150 - accuracy: 0.8105 - val_loss: 0.5002 - val_accuracy: 0.7530\n",
            "Epoch 83/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.4049 - accuracy: 0.8201 - val_loss: 0.4961 - val_accuracy: 0.7490\n",
            "Epoch 84/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.4017 - accuracy: 0.8407 - val_loss: 0.5095 - val_accuracy: 0.7045\n",
            "Epoch 85/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.3886 - accuracy: 0.8354 - val_loss: 0.4983 - val_accuracy: 0.7490\n",
            "Epoch 86/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.3987 - accuracy: 0.8346 - val_loss: 0.4956 - val_accuracy: 0.7490\n",
            "Epoch 87/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.3840 - accuracy: 0.8407 - val_loss: 0.4892 - val_accuracy: 0.7571\n",
            "Epoch 88/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.3807 - accuracy: 0.8477 - val_loss: 0.5031 - val_accuracy: 0.7166\n",
            "Epoch 89/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.3827 - accuracy: 0.8438 - val_loss: 0.4932 - val_accuracy: 0.7449\n",
            "Epoch 90/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.3760 - accuracy: 0.8473 - val_loss: 0.4953 - val_accuracy: 0.7328\n",
            "Epoch 91/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.3701 - accuracy: 0.8551 - val_loss: 0.4925 - val_accuracy: 0.7449\n",
            "Epoch 92/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.3629 - accuracy: 0.8516 - val_loss: 0.4930 - val_accuracy: 0.7409\n",
            "Epoch 93/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.3699 - accuracy: 0.8499 - val_loss: 0.4990 - val_accuracy: 0.7409\n",
            "Epoch 94/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.3545 - accuracy: 0.8683 - val_loss: 0.4979 - val_accuracy: 0.7206\n",
            "Epoch 95/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.3666 - accuracy: 0.8547 - val_loss: 0.4918 - val_accuracy: 0.7368\n",
            "Epoch 96/300\n",
            "143/143 [==============================] - 8s 57ms/step - loss: 0.3587 - accuracy: 0.8648 - val_loss: 0.4897 - val_accuracy: 0.7611\n",
            "Epoch 97/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.3494 - accuracy: 0.8591 - val_loss: 0.4979 - val_accuracy: 0.7328\n",
            "Epoch 98/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.3448 - accuracy: 0.8639 - val_loss: 0.4873 - val_accuracy: 0.7409\n",
            "Epoch 99/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.3470 - accuracy: 0.8573 - val_loss: 0.4914 - val_accuracy: 0.7368\n",
            "Epoch 100/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.3471 - accuracy: 0.8543 - val_loss: 0.4877 - val_accuracy: 0.7449\n",
            "Epoch 101/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.3325 - accuracy: 0.8661 - val_loss: 0.4887 - val_accuracy: 0.7571\n",
            "Epoch 102/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.3358 - accuracy: 0.8766 - val_loss: 0.4972 - val_accuracy: 0.7287\n",
            "Epoch 103/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.3316 - accuracy: 0.8656 - val_loss: 0.4842 - val_accuracy: 0.7449\n",
            "Epoch 104/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.3155 - accuracy: 0.8871 - val_loss: 0.4906 - val_accuracy: 0.7530\n",
            "Epoch 105/300\n",
            "143/143 [==============================] - 8s 57ms/step - loss: 0.3198 - accuracy: 0.8801 - val_loss: 0.4849 - val_accuracy: 0.7652\n",
            "Epoch 106/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.3201 - accuracy: 0.8805 - val_loss: 0.4950 - val_accuracy: 0.7571\n",
            "Epoch 107/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.3001 - accuracy: 0.8818 - val_loss: 0.4883 - val_accuracy: 0.7449\n",
            "Epoch 108/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.3173 - accuracy: 0.8805 - val_loss: 0.4917 - val_accuracy: 0.7206\n",
            "Epoch 109/300\n",
            "143/143 [==============================] - 8s 58ms/step - loss: 0.3186 - accuracy: 0.8722 - val_loss: 0.4846 - val_accuracy: 0.7692\n",
            "Epoch 110/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.2982 - accuracy: 0.8832 - val_loss: 0.4954 - val_accuracy: 0.7247\n",
            "Epoch 111/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.3033 - accuracy: 0.8862 - val_loss: 0.4871 - val_accuracy: 0.7287\n",
            "Epoch 112/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.2998 - accuracy: 0.8832 - val_loss: 0.4922 - val_accuracy: 0.7449\n",
            "Epoch 113/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.2913 - accuracy: 0.8867 - val_loss: 0.5006 - val_accuracy: 0.7166\n",
            "Epoch 114/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.2919 - accuracy: 0.8928 - val_loss: 0.4933 - val_accuracy: 0.7287\n",
            "Epoch 115/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.2791 - accuracy: 0.9007 - val_loss: 0.4870 - val_accuracy: 0.7490\n",
            "Epoch 116/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.2894 - accuracy: 0.8945 - val_loss: 0.4844 - val_accuracy: 0.7287\n",
            "Epoch 117/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.2890 - accuracy: 0.8932 - val_loss: 0.4951 - val_accuracy: 0.7206\n",
            "Epoch 118/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.2807 - accuracy: 0.8989 - val_loss: 0.4852 - val_accuracy: 0.7530\n",
            "Epoch 119/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.2698 - accuracy: 0.8976 - val_loss: 0.4888 - val_accuracy: 0.7287\n",
            "Epoch 120/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.2694 - accuracy: 0.8993 - val_loss: 0.4897 - val_accuracy: 0.7449\n",
            "Epoch 121/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.2743 - accuracy: 0.9037 - val_loss: 0.4984 - val_accuracy: 0.7166\n",
            "Epoch 122/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.2689 - accuracy: 0.8989 - val_loss: 0.4963 - val_accuracy: 0.7247\n",
            "Epoch 123/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.2648 - accuracy: 0.9002 - val_loss: 0.5061 - val_accuracy: 0.7206\n",
            "Epoch 124/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.2671 - accuracy: 0.9002 - val_loss: 0.4926 - val_accuracy: 0.7287\n",
            "Epoch 125/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.2556 - accuracy: 0.9107 - val_loss: 0.4892 - val_accuracy: 0.7490\n",
            "Epoch 126/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.2499 - accuracy: 0.9168 - val_loss: 0.4951 - val_accuracy: 0.7490\n",
            "Epoch 127/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.2528 - accuracy: 0.9033 - val_loss: 0.5017 - val_accuracy: 0.7368\n",
            "Epoch 128/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.2539 - accuracy: 0.9059 - val_loss: 0.4893 - val_accuracy: 0.7449\n",
            "Epoch 129/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.2514 - accuracy: 0.9015 - val_loss: 0.4933 - val_accuracy: 0.7328\n",
            "Epoch 130/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.2537 - accuracy: 0.9046 - val_loss: 0.4893 - val_accuracy: 0.7409\n",
            "Epoch 131/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.2325 - accuracy: 0.9186 - val_loss: 0.4901 - val_accuracy: 0.7611\n",
            "Epoch 132/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.2418 - accuracy: 0.9142 - val_loss: 0.4884 - val_accuracy: 0.7611\n",
            "Epoch 133/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.2412 - accuracy: 0.9125 - val_loss: 0.4931 - val_accuracy: 0.7490\n",
            "Epoch 134/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.2329 - accuracy: 0.9195 - val_loss: 0.4953 - val_accuracy: 0.7368\n",
            "Epoch 135/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.2379 - accuracy: 0.9107 - val_loss: 0.4957 - val_accuracy: 0.7652\n",
            "Epoch 136/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.2376 - accuracy: 0.9120 - val_loss: 0.4942 - val_accuracy: 0.7409\n",
            "Epoch 137/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.2246 - accuracy: 0.9252 - val_loss: 0.4917 - val_accuracy: 0.7368\n",
            "Epoch 138/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.2165 - accuracy: 0.9234 - val_loss: 0.5008 - val_accuracy: 0.7490\n",
            "Epoch 139/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.2156 - accuracy: 0.9265 - val_loss: 0.5017 - val_accuracy: 0.7328\n",
            "Epoch 140/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.2190 - accuracy: 0.9234 - val_loss: 0.4981 - val_accuracy: 0.7409\n",
            "Epoch 141/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.2171 - accuracy: 0.9234 - val_loss: 0.5059 - val_accuracy: 0.7328\n",
            "Epoch 142/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.2186 - accuracy: 0.9239 - val_loss: 0.5042 - val_accuracy: 0.7409\n",
            "Epoch 143/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.2090 - accuracy: 0.9300 - val_loss: 0.5015 - val_accuracy: 0.7328\n",
            "Epoch 144/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1985 - accuracy: 0.9330 - val_loss: 0.5022 - val_accuracy: 0.7449\n",
            "Epoch 145/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.2012 - accuracy: 0.9335 - val_loss: 0.5041 - val_accuracy: 0.7368\n",
            "Epoch 146/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.2042 - accuracy: 0.9335 - val_loss: 0.5009 - val_accuracy: 0.7409\n",
            "Epoch 147/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.2011 - accuracy: 0.9339 - val_loss: 0.5060 - val_accuracy: 0.7368\n",
            "Epoch 148/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.1972 - accuracy: 0.9287 - val_loss: 0.5150 - val_accuracy: 0.7530\n",
            "Epoch 149/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1952 - accuracy: 0.9335 - val_loss: 0.5159 - val_accuracy: 0.7409\n",
            "Epoch 150/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1859 - accuracy: 0.9383 - val_loss: 0.5088 - val_accuracy: 0.7449\n",
            "Epoch 151/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1945 - accuracy: 0.9365 - val_loss: 0.5152 - val_accuracy: 0.7409\n",
            "Epoch 152/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.1929 - accuracy: 0.9344 - val_loss: 0.5115 - val_accuracy: 0.7611\n",
            "Epoch 153/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1796 - accuracy: 0.9488 - val_loss: 0.5161 - val_accuracy: 0.7449\n",
            "Epoch 154/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1794 - accuracy: 0.9453 - val_loss: 0.5136 - val_accuracy: 0.7449\n",
            "Epoch 155/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1826 - accuracy: 0.9348 - val_loss: 0.5257 - val_accuracy: 0.7449\n",
            "Epoch 156/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1766 - accuracy: 0.9427 - val_loss: 0.5191 - val_accuracy: 0.7571\n",
            "Epoch 157/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1816 - accuracy: 0.9370 - val_loss: 0.5249 - val_accuracy: 0.7490\n",
            "Epoch 158/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1851 - accuracy: 0.9387 - val_loss: 0.5147 - val_accuracy: 0.7328\n",
            "Epoch 159/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1811 - accuracy: 0.9383 - val_loss: 0.5206 - val_accuracy: 0.7409\n",
            "Epoch 160/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1809 - accuracy: 0.9435 - val_loss: 0.5186 - val_accuracy: 0.7368\n",
            "Epoch 161/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1724 - accuracy: 0.9405 - val_loss: 0.5236 - val_accuracy: 0.7490\n",
            "Epoch 162/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1721 - accuracy: 0.9427 - val_loss: 0.5133 - val_accuracy: 0.7449\n",
            "Epoch 163/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1603 - accuracy: 0.9549 - val_loss: 0.5242 - val_accuracy: 0.7571\n",
            "Epoch 164/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1704 - accuracy: 0.9405 - val_loss: 0.5286 - val_accuracy: 0.7368\n",
            "Epoch 165/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1611 - accuracy: 0.9470 - val_loss: 0.5305 - val_accuracy: 0.7368\n",
            "Epoch 166/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1602 - accuracy: 0.9479 - val_loss: 0.5257 - val_accuracy: 0.7449\n",
            "Epoch 167/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1701 - accuracy: 0.9435 - val_loss: 0.5268 - val_accuracy: 0.7328\n",
            "Epoch 168/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1591 - accuracy: 0.9540 - val_loss: 0.5324 - val_accuracy: 0.7368\n",
            "Epoch 169/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1601 - accuracy: 0.9484 - val_loss: 0.5309 - val_accuracy: 0.7449\n",
            "Epoch 170/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1536 - accuracy: 0.9501 - val_loss: 0.5340 - val_accuracy: 0.7449\n",
            "Epoch 171/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1574 - accuracy: 0.9505 - val_loss: 0.5357 - val_accuracy: 0.7571\n",
            "Epoch 172/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1511 - accuracy: 0.9519 - val_loss: 0.5336 - val_accuracy: 0.7409\n",
            "Epoch 173/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1502 - accuracy: 0.9479 - val_loss: 0.5414 - val_accuracy: 0.7652\n",
            "Epoch 174/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1435 - accuracy: 0.9602 - val_loss: 0.5397 - val_accuracy: 0.7530\n",
            "Epoch 175/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1432 - accuracy: 0.9567 - val_loss: 0.5477 - val_accuracy: 0.7530\n",
            "Epoch 176/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1462 - accuracy: 0.9540 - val_loss: 0.5349 - val_accuracy: 0.7409\n",
            "Epoch 177/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1468 - accuracy: 0.9589 - val_loss: 0.5414 - val_accuracy: 0.7530\n",
            "Epoch 178/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1368 - accuracy: 0.9606 - val_loss: 0.5452 - val_accuracy: 0.7490\n",
            "Epoch 179/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1396 - accuracy: 0.9589 - val_loss: 0.5379 - val_accuracy: 0.7652\n",
            "Epoch 180/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1330 - accuracy: 0.9597 - val_loss: 0.5380 - val_accuracy: 0.7530\n",
            "Epoch 181/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1297 - accuracy: 0.9650 - val_loss: 0.5385 - val_accuracy: 0.7449\n",
            "Epoch 182/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1356 - accuracy: 0.9637 - val_loss: 0.5463 - val_accuracy: 0.7449\n",
            "Epoch 183/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1303 - accuracy: 0.9575 - val_loss: 0.5443 - val_accuracy: 0.7530\n",
            "Epoch 184/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1290 - accuracy: 0.9650 - val_loss: 0.5466 - val_accuracy: 0.7692\n",
            "Epoch 185/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1304 - accuracy: 0.9589 - val_loss: 0.5582 - val_accuracy: 0.7449\n",
            "Epoch 186/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1244 - accuracy: 0.9659 - val_loss: 0.5444 - val_accuracy: 0.7490\n",
            "Epoch 187/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1227 - accuracy: 0.9659 - val_loss: 0.5446 - val_accuracy: 0.7490\n",
            "Epoch 188/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.1252 - accuracy: 0.9597 - val_loss: 0.5648 - val_accuracy: 0.7449\n",
            "Epoch 189/300\n",
            "143/143 [==============================] - 8s 54ms/step - loss: 0.1197 - accuracy: 0.9602 - val_loss: 0.5494 - val_accuracy: 0.7449\n",
            "Epoch 190/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1248 - accuracy: 0.9593 - val_loss: 0.5563 - val_accuracy: 0.7449\n",
            "Epoch 191/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1170 - accuracy: 0.9689 - val_loss: 0.5576 - val_accuracy: 0.7530\n",
            "Epoch 192/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1306 - accuracy: 0.9602 - val_loss: 0.5588 - val_accuracy: 0.7571\n",
            "Epoch 193/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1218 - accuracy: 0.9637 - val_loss: 0.5584 - val_accuracy: 0.7530\n",
            "Epoch 194/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1186 - accuracy: 0.9611 - val_loss: 0.5589 - val_accuracy: 0.7530\n",
            "Epoch 195/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1159 - accuracy: 0.9698 - val_loss: 0.5575 - val_accuracy: 0.7571\n",
            "Epoch 196/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1126 - accuracy: 0.9689 - val_loss: 0.5693 - val_accuracy: 0.7449\n",
            "Epoch 197/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1085 - accuracy: 0.9707 - val_loss: 0.5648 - val_accuracy: 0.7449\n",
            "Epoch 198/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1073 - accuracy: 0.9707 - val_loss: 0.5662 - val_accuracy: 0.7530\n",
            "Epoch 199/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1018 - accuracy: 0.9737 - val_loss: 0.5746 - val_accuracy: 0.7530\n",
            "Epoch 200/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1056 - accuracy: 0.9702 - val_loss: 0.5731 - val_accuracy: 0.7530\n",
            "Epoch 201/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1106 - accuracy: 0.9676 - val_loss: 0.5718 - val_accuracy: 0.7490\n",
            "Epoch 202/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1023 - accuracy: 0.9720 - val_loss: 0.5758 - val_accuracy: 0.7490\n",
            "Epoch 203/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1081 - accuracy: 0.9676 - val_loss: 0.5775 - val_accuracy: 0.7449\n",
            "Epoch 204/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1045 - accuracy: 0.9720 - val_loss: 0.5953 - val_accuracy: 0.7449\n",
            "Epoch 205/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.0989 - accuracy: 0.9768 - val_loss: 0.5848 - val_accuracy: 0.7571\n",
            "Epoch 206/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1093 - accuracy: 0.9646 - val_loss: 0.5877 - val_accuracy: 0.7490\n",
            "Epoch 207/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.1037 - accuracy: 0.9646 - val_loss: 0.5999 - val_accuracy: 0.7530\n",
            "Epoch 208/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.0980 - accuracy: 0.9724 - val_loss: 0.6022 - val_accuracy: 0.7490\n",
            "Epoch 209/300\n",
            "143/143 [==============================] - 8s 53ms/step - loss: 0.0976 - accuracy: 0.9729 - val_loss: 0.5916 - val_accuracy: 0.7409\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[[1.89380292e-02 2.66220421e-02]\n",
            " [9.33439359e-02 2.92497245e-03]\n",
            " [8.42635706e-02 1.54422410e-03]\n",
            " [6.88970834e-02 2.78981612e-03]\n",
            " [6.66370168e-02 2.33437959e-03]\n",
            " [1.04806885e-01 2.44096364e-03]\n",
            " [7.89373219e-02 2.10208120e-03]\n",
            " [9.00132284e-02 2.27260427e-03]\n",
            " [2.50202399e-02 5.41830203e-03]\n",
            " [3.83084714e-02 7.41844717e-03]\n",
            " [3.16670425e-02 3.73918936e-03]\n",
            " [7.53960162e-02 2.94234836e-03]\n",
            " [3.42641212e-02 2.87251570e-03]\n",
            " [4.06012088e-02 4.90869349e-03]\n",
            " [5.04006930e-02 3.06443055e-03]\n",
            " [7.99212456e-02 3.70004238e-03]\n",
            " [4.64081429e-02 1.42826478e-03]\n",
            " [1.06156468e-01 9.00302455e-03]\n",
            " [1.09848134e-01 1.12575684e-02]\n",
            " [2.03483403e-01 1.54780429e-02]\n",
            " [9.81301889e-02 1.45785362e-02]\n",
            " [9.45720226e-02 1.65661760e-02]\n",
            " [8.45316797e-02 1.46987429e-02]\n",
            " [1.34383053e-01 7.06416601e-03]\n",
            " [4.94938008e-02 1.26794055e-02]\n",
            " [3.98557168e-03 5.13775134e-03]\n",
            " [1.41985454e-02 5.13650570e-03]\n",
            " [2.37745065e-02 2.64991191e-03]\n",
            " [1.05711907e-01 7.13879941e-03]\n",
            " [7.83469081e-02 1.45253716e-02]\n",
            " [1.02615304e-01 1.42007545e-02]\n",
            " [6.15297668e-02 4.53030458e-03]\n",
            " [1.03367092e-02 5.19413268e-04]\n",
            " [5.14919451e-03 1.78125664e-03]\n",
            " [1.80137381e-02 2.94553931e-03]\n",
            " [5.64322509e-02 8.21894314e-03]\n",
            " [4.17567790e-02 8.06404371e-03]\n",
            " [3.84480692e-02 3.11100967e-02]\n",
            " [2.35364512e-02 9.44311172e-03]\n",
            " [5.10793142e-02 8.93137604e-03]\n",
            " [2.72924406e-03 2.41091894e-03]\n",
            " [4.89130896e-03 2.52873125e-03]\n",
            " [2.09678188e-02 1.09579964e-02]\n",
            " [1.94483064e-02 1.33633064e-02]\n",
            " [3.47733721e-02 7.04703573e-03]\n",
            " [4.21287529e-02 8.74933228e-03]\n",
            " [5.43237217e-02 4.50654654e-03]\n",
            " [5.20219989e-02 2.25160946e-03]\n",
            " [7.99557865e-02 7.53671536e-03]\n",
            " [7.57163391e-02 6.64779125e-03]\n",
            " [4.72695939e-02 7.63657829e-03]\n",
            " [5.20458259e-02 9.37225204e-03]\n",
            " [5.72126657e-02 1.18510928e-02]\n",
            " [2.66791023e-02 2.53356099e-02]\n",
            " [5.60023263e-02 6.49667205e-03]\n",
            " [5.03791794e-02 7.36020412e-03]\n",
            " [3.94108556e-02 8.76519457e-03]\n",
            " [6.14107214e-02 8.70168861e-03]\n",
            " [2.54647713e-02 3.96763347e-03]\n",
            " [3.29011194e-02 4.31634346e-03]\n",
            " [1.20688684e-01 1.36020901e-02]\n",
            " [9.86363143e-02 4.01972383e-02]\n",
            " [1.01883514e-02 1.70305911e-02]\n",
            " [7.03082159e-02 1.87714454e-02]\n",
            " [9.60187390e-02 2.56822556e-02]\n",
            " [7.00531527e-02 4.16337177e-02]\n",
            " [6.61771186e-03 2.05489006e-02]\n",
            " [9.50544141e-03 4.71279863e-03]\n",
            " [5.86944371e-02 4.77660960e-03]\n",
            " [3.51615548e-02 1.63327884e-02]\n",
            " [7.30568916e-02 6.80577159e-02]\n",
            " [2.10806821e-02 7.03328196e-03]\n",
            " [2.71013062e-02 5.22597805e-02]\n",
            " [6.31211512e-03 7.40437657e-02]\n",
            " [9.75162350e-03 5.39742671e-02]\n",
            " [2.20733155e-02 4.61128466e-02]\n",
            " [9.43730585e-03 9.89199951e-02]\n",
            " [5.27286064e-03 3.66521627e-02]\n",
            " [8.55614617e-03 5.12573756e-02]\n",
            " [1.95775125e-02 4.94179241e-02]\n",
            " [1.07335271e-02 4.06763144e-02]\n",
            " [5.06536430e-03 2.42577177e-02]\n",
            " [7.03881495e-03 4.57127988e-02]\n",
            " [1.51517205e-02 9.72250775e-02]\n",
            " [7.49055017e-03 5.78765757e-02]\n",
            " [9.25679877e-03 1.05576679e-01]\n",
            " [5.54343201e-02 5.36768772e-02]\n",
            " [2.22483873e-02 2.30848752e-02]\n",
            " [1.99660572e-04 1.69142317e-02]\n",
            " [1.83916856e-02 2.86234468e-02]\n",
            " [1.51418392e-02 2.04606783e-02]\n",
            " [2.03336459e-02 1.64608322e-02]\n",
            " [1.40125239e-02 2.58205701e-02]\n",
            " [1.03504062e-02 3.53709087e-02]\n",
            " [2.21289676e-02 1.67859625e-02]\n",
            " [1.37284519e-02 2.57624015e-02]\n",
            " [2.21398603e-02 2.18548458e-02]\n",
            " [2.93622967e-02 5.45446053e-02]\n",
            " [2.88802683e-02 1.21165495e-02]\n",
            " [2.34812815e-02 2.10014936e-02]\n",
            " [4.13344763e-02 2.53719129e-02]\n",
            " [1.51975835e-02 2.81611197e-02]\n",
            " [3.32157575e-02 3.18376571e-02]\n",
            " [2.23546270e-02 4.51772138e-02]\n",
            " [1.60323754e-02 2.35062931e-02]\n",
            " [1.92880221e-02 3.07103880e-02]\n",
            " [2.75963359e-02 4.39465307e-02]\n",
            " [1.72522254e-02 2.65909471e-02]\n",
            " [4.32145447e-02 2.01756824e-02]\n",
            " [2.92063057e-02 7.76492152e-03]\n",
            " [2.59335991e-03 1.64382649e-03]\n",
            " [7.79113080e-03 8.39001685e-03]\n",
            " [1.53928502e-02 8.10399931e-03]\n",
            " [1.79574545e-02 8.18945933e-03]\n",
            " [3.28465924e-02 1.84513163e-02]\n",
            " [2.51222868e-02 1.00841541e-02]\n",
            " [3.37184109e-02 6.13605529e-02]\n",
            " [1.05486121e-02 1.60730071e-02]\n",
            " [1.62329078e-02 4.98779956e-03]\n",
            " [2.50781029e-02 1.23776160e-02]\n",
            " [1.04027214e-02 4.30485792e-03]\n",
            " [6.41957915e-04 4.32697013e-02]\n",
            " [1.92570069e-03 5.02851605e-02]\n",
            " [7.88806938e-03 1.12969682e-01]\n",
            " [3.11695621e-03 7.50902668e-02]\n",
            " [5.18444227e-03 4.46736477e-02]\n",
            " [3.21046798e-03 3.00255939e-02]\n",
            " [7.63330841e-04 3.37414257e-02]\n",
            " [5.63295698e-03 1.21914279e-02]\n",
            " [1.03369448e-02 4.64137867e-02]\n",
            " [2.72162841e-03 3.17882486e-02]\n",
            " [4.20152163e-03 3.35900187e-02]\n",
            " [3.53878061e-03 4.42195237e-02]\n",
            " [2.33628694e-03 3.64563391e-02]\n",
            " [2.81044660e-04 1.29134515e-02]\n",
            " [5.10680815e-03 7.06138685e-02]\n",
            " [1.40237026e-02 5.89404255e-02]\n",
            " [1.00248791e-02 5.76590411e-02]\n",
            " [1.09605845e-02 5.09285554e-02]\n",
            " [7.03515718e-03 3.50811966e-02]\n",
            " [7.50902062e-03 2.05385275e-02]\n",
            " [1.13360547e-02 4.33586389e-02]\n",
            " [7.04938173e-03 3.26224715e-02]\n",
            " [7.41841923e-03 3.87027413e-02]\n",
            " [4.66359500e-03 4.66678180e-02]\n",
            " [4.80783591e-03 1.83451660e-02]\n",
            " [8.48934799e-03 4.58944328e-02]\n",
            " [6.80619560e-04 7.86935445e-03]\n",
            " [7.68781319e-06 3.49918101e-03]\n",
            " [1.23993838e-02 2.07993225e-03]\n",
            " [2.04664040e-02 4.32321616e-03]\n",
            " [1.18673369e-02 2.56941584e-03]\n",
            " [1.89266410e-02 5.46132307e-03]\n",
            " [2.92756204e-02 3.94532830e-03]\n",
            " [2.13201400e-02 3.27281607e-03]\n",
            " [3.01527772e-02 3.87004181e-03]\n",
            " [2.49629337e-02 1.14877382e-02]\n",
            " [8.71229451e-03 4.09508161e-02]\n",
            " [7.81511609e-03 2.16838811e-02]\n",
            " [4.75201830e-02 1.99616309e-02]\n",
            " [7.59859607e-02 1.52032636e-02]\n",
            " [4.42404822e-02 9.58851818e-03]\n",
            " [2.47403271e-02 1.11432942e-02]\n",
            " [3.10367327e-02 6.78533828e-03]\n",
            " [1.80107467e-02 1.91465393e-03]\n",
            " [1.54686943e-02 2.04828405e-03]\n",
            " [5.53648919e-04 9.81186866e-04]\n",
            " [2.04664432e-02 1.60909034e-02]\n",
            " [1.15191378e-02 1.05905114e-02]\n",
            " [2.10260740e-03 5.05918916e-03]\n",
            " [1.59554137e-03 5.89602208e-03]\n",
            " [1.52078690e-02 1.36371255e-02]\n",
            " [5.32383099e-03 2.06511628e-04]\n",
            " [1.85382587e-03 5.08463942e-04]\n",
            " [2.50200601e-03 8.48784810e-04]\n",
            " [1.42162619e-02 1.74452097e-03]\n",
            " [1.71038415e-02 3.32925958e-03]\n",
            " [7.44014531e-02 6.71480084e-03]\n",
            " [1.30506501e-01 2.62235571e-02]\n",
            " [1.27088562e-01 9.18445587e-02]\n",
            " [1.72452793e-01 5.58828861e-02]\n",
            " [1.06793851e-01 1.03935357e-02]\n",
            " [5.10576833e-03 1.96174276e-03]\n",
            " [3.74871748e-03 3.94033210e-04]\n",
            " [1.14642153e-03 7.55341971e-05]\n",
            " [2.75400449e-02 7.18345036e-05]\n",
            " [4.91763242e-02 3.19825311e-04]\n",
            " [1.38701275e-01 4.67749359e-03]\n",
            " [1.48849592e-01 1.09843267e-02]\n",
            " [8.95159319e-02 5.70306741e-03]\n",
            " [1.01711847e-01 3.97618441e-03]\n",
            " [9.08875987e-02 5.59390290e-03]\n",
            " [3.92307304e-02 8.88678897e-03]\n",
            " [2.94349045e-02 1.06029939e-02]\n",
            " [2.62551904e-02 2.56884638e-02]\n",
            " [7.51513988e-02 6.97137741e-03]\n",
            " [3.74917798e-02 1.19191939e-02]\n",
            " [8.81759375e-02 6.98774680e-03]\n",
            " [8.92766416e-02 5.98995807e-03]\n",
            " [1.20738558e-01 3.21677630e-03]\n",
            " [1.03651367e-01 1.51014898e-03]\n",
            " [1.57571305e-02 2.83916667e-03]\n",
            " [5.86584508e-02 1.93658117e-02]\n",
            " [9.76744518e-02 9.17578936e-02]\n",
            " [2.69369259e-02 2.75511108e-02]\n",
            " [6.07431121e-03 3.81263569e-02]\n",
            " [2.33776239e-03 1.51135977e-02]\n",
            " [2.39326227e-02 5.99208213e-02]\n",
            " [2.85244100e-02 1.20006613e-01]\n",
            " [6.55196235e-02 1.44483015e-01]\n",
            " [2.33698898e-04 5.75865619e-04]\n",
            " [6.44961023e-04 8.31715856e-03]\n",
            " [4.19387780e-03 2.35984158e-02]\n",
            " [2.45490251e-03 3.63069698e-02]\n",
            " [1.62851636e-03 2.85244752e-02]\n",
            " [3.78365777e-02 1.36018097e-02]\n",
            " [3.29816388e-03 3.19390073e-02]\n",
            " [9.93364677e-03 1.57486144e-02]\n",
            " [3.32292132e-02 7.69185424e-02]\n",
            " [6.47192895e-02 4.17346172e-02]\n",
            " [2.41058264e-02 7.64569566e-02]\n",
            " [5.39364293e-02 8.13423768e-02]\n",
            " [1.42140845e-02 3.21405530e-02]\n",
            " [5.84083050e-02 5.03495410e-02]\n",
            " [6.12467751e-02 6.00692071e-02]\n",
            " [2.59186253e-02 6.42420948e-02]\n",
            " [1.26494318e-02 1.28673576e-02]\n",
            " [1.21029001e-02 8.43906496e-03]\n",
            " [6.27496839e-02 3.10286731e-02]\n",
            " [7.29557574e-02 3.77088562e-02]\n",
            " [5.92211448e-02 9.34838429e-02]\n",
            " [3.73774990e-02 4.81153652e-02]\n",
            " [6.59390980e-06 1.32635888e-03]\n",
            " [8.06927710e-05 1.85128655e-02]\n",
            " [1.30292857e-02 1.04047179e-01]\n",
            " [5.54385409e-03 1.10622630e-01]\n",
            " [3.12628085e-03 3.36091071e-02]\n",
            " [2.05497835e-02 4.79491539e-02]\n",
            " [1.10424971e-02 1.25830844e-01]\n",
            " [8.38734675e-03 1.41722411e-01]\n",
            " [2.32668081e-03 4.64105047e-02]\n",
            " [3.13505568e-02 1.06708869e-01]\n",
            " [1.15084797e-02 4.75224964e-02]\n",
            " [5.17566735e-03 6.55523911e-02]\n",
            " [1.89175233e-02 1.38452306e-01]\n",
            " [1.34042706e-02 1.93849444e-01]\n",
            " [2.03737160e-04 3.35673392e-02]]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0 0]\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "[0 0 0 0 1 0 0 0 1 0 0 0 0 0]\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1]\n",
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "[1 1 1 0 1 1 0 1 0 1]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 1 0 1 1 1 1 1 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0 1 0 0 0 0 1 1 0 0 0]\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "[0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0]\n",
            "[1 0 0 1 1 0]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[1 1 1 1 1 1 1 1]\n",
            "[0 0 1 1 1 1 1 1]\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "[1 1 1 1 1 0 1 1 1 0 1 1 1]\n",
            "[0 0 0 0 0 0 0 0 0]\n",
            "[0 0 1 1 0 0 0 1 1]\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "Correct Predictions: 16, incorrect predictions: 3\n",
            "Starting 3\n",
            "128\n",
            "1292\n",
            "1\n",
            "(None, 128, 1292, 1)\n",
            "(None, 1, 323, 92)\n",
            "\n",
            " Reshape\n",
            "(None, 92, 323, 1)\n",
            "\n",
            " Four\n",
            "(None, 1, 316, 32)\n",
            "\n",
            " Five\n",
            "(None, 1, 79, 32)\n",
            "(None, 32, 79, 1)\n",
            "Epoch 1/300\n",
            "  2/142 [..............................] - ETA: 3s - loss: 0.6930 - accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_train_batch_end` time: 0.0486s). Check your callbacks.\n",
            "142/142 [==============================] - 8s 58ms/step - loss: 0.6940 - accuracy: 0.5013 - val_loss: 0.6909 - val_accuracy: 0.5423\n",
            "Epoch 2/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.6916 - accuracy: 0.5502 - val_loss: 0.6897 - val_accuracy: 0.5423\n",
            "Epoch 3/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.6899 - accuracy: 0.5458 - val_loss: 0.6893 - val_accuracy: 0.5423\n",
            "Epoch 4/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.6898 - accuracy: 0.5462 - val_loss: 0.6889 - val_accuracy: 0.5423\n",
            "Epoch 5/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.6884 - accuracy: 0.5550 - val_loss: 0.6886 - val_accuracy: 0.5423\n",
            "Epoch 6/300\n",
            "142/142 [==============================] - 7s 52ms/step - loss: 0.6863 - accuracy: 0.5555 - val_loss: 0.6885 - val_accuracy: 0.5423\n",
            "Epoch 7/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.6872 - accuracy: 0.5502 - val_loss: 0.6881 - val_accuracy: 0.5423\n",
            "Epoch 8/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.6860 - accuracy: 0.5515 - val_loss: 0.6877 - val_accuracy: 0.5423\n",
            "Epoch 9/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.6838 - accuracy: 0.5678 - val_loss: 0.6879 - val_accuracy: 0.5423\n",
            "Epoch 10/300\n",
            "142/142 [==============================] - 8s 56ms/step - loss: 0.6843 - accuracy: 0.5577 - val_loss: 0.6873 - val_accuracy: 0.5462\n",
            "Epoch 11/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.6847 - accuracy: 0.5616 - val_loss: 0.6869 - val_accuracy: 0.5500\n",
            "Epoch 12/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.6825 - accuracy: 0.5599 - val_loss: 0.6872 - val_accuracy: 0.5500\n",
            "Epoch 13/300\n",
            "142/142 [==============================] - 8s 57ms/step - loss: 0.6815 - accuracy: 0.5700 - val_loss: 0.6867 - val_accuracy: 0.5577\n",
            "Epoch 14/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.6782 - accuracy: 0.5748 - val_loss: 0.6868 - val_accuracy: 0.5577\n",
            "Epoch 15/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.6800 - accuracy: 0.5616 - val_loss: 0.6860 - val_accuracy: 0.5577\n",
            "Epoch 16/300\n",
            "142/142 [==============================] - 8s 57ms/step - loss: 0.6781 - accuracy: 0.5757 - val_loss: 0.6857 - val_accuracy: 0.5615\n",
            "Epoch 17/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.6781 - accuracy: 0.5823 - val_loss: 0.6856 - val_accuracy: 0.5577\n",
            "Epoch 18/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.6760 - accuracy: 0.5682 - val_loss: 0.6850 - val_accuracy: 0.5577\n",
            "Epoch 19/300\n",
            "142/142 [==============================] - 7s 52ms/step - loss: 0.6726 - accuracy: 0.5810 - val_loss: 0.6847 - val_accuracy: 0.5577\n",
            "Epoch 20/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.6716 - accuracy: 0.5902 - val_loss: 0.6841 - val_accuracy: 0.5615\n",
            "Epoch 21/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.6708 - accuracy: 0.5915 - val_loss: 0.6830 - val_accuracy: 0.5538\n",
            "Epoch 22/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.6697 - accuracy: 0.5999 - val_loss: 0.6835 - val_accuracy: 0.5577\n",
            "Epoch 23/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.6633 - accuracy: 0.6109 - val_loss: 0.6815 - val_accuracy: 0.5500\n",
            "Epoch 24/300\n",
            "142/142 [==============================] - 8s 56ms/step - loss: 0.6624 - accuracy: 0.6092 - val_loss: 0.6803 - val_accuracy: 0.5692\n",
            "Epoch 25/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.6595 - accuracy: 0.6171 - val_loss: 0.6801 - val_accuracy: 0.5692\n",
            "Epoch 26/300\n",
            "142/142 [==============================] - 7s 52ms/step - loss: 0.6567 - accuracy: 0.6254 - val_loss: 0.6796 - val_accuracy: 0.5577\n",
            "Epoch 27/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.6474 - accuracy: 0.6351 - val_loss: 0.6783 - val_accuracy: 0.5615\n",
            "Epoch 28/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.6477 - accuracy: 0.6312 - val_loss: 0.6759 - val_accuracy: 0.5577\n",
            "Epoch 29/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.6449 - accuracy: 0.6391 - val_loss: 0.6756 - val_accuracy: 0.5538\n",
            "Epoch 30/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.6395 - accuracy: 0.6554 - val_loss: 0.6750 - val_accuracy: 0.5654\n",
            "Epoch 31/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.6285 - accuracy: 0.6664 - val_loss: 0.6738 - val_accuracy: 0.5654\n",
            "Epoch 32/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.6301 - accuracy: 0.6395 - val_loss: 0.6747 - val_accuracy: 0.5500\n",
            "Epoch 33/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.6284 - accuracy: 0.6646 - val_loss: 0.6739 - val_accuracy: 0.5500\n",
            "Epoch 34/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.6162 - accuracy: 0.6778 - val_loss: 0.6746 - val_accuracy: 0.5423\n",
            "Epoch 35/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.6089 - accuracy: 0.6840 - val_loss: 0.6770 - val_accuracy: 0.5346\n",
            "Epoch 36/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.6075 - accuracy: 0.6796 - val_loss: 0.6743 - val_accuracy: 0.5462\n",
            "Epoch 37/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.6032 - accuracy: 0.6928 - val_loss: 0.6749 - val_accuracy: 0.5462\n",
            "Epoch 38/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.5922 - accuracy: 0.7082 - val_loss: 0.6749 - val_accuracy: 0.5385\n",
            "Epoch 39/300\n",
            "142/142 [==============================] - 8s 56ms/step - loss: 0.5902 - accuracy: 0.7082 - val_loss: 0.6708 - val_accuracy: 0.5731\n",
            "Epoch 40/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.5905 - accuracy: 0.7077 - val_loss: 0.6711 - val_accuracy: 0.5885\n",
            "Epoch 41/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.5814 - accuracy: 0.7201 - val_loss: 0.6728 - val_accuracy: 0.5846\n",
            "Epoch 42/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.5770 - accuracy: 0.7148 - val_loss: 0.6748 - val_accuracy: 0.5654\n",
            "Epoch 43/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.5687 - accuracy: 0.7223 - val_loss: 0.6730 - val_accuracy: 0.5885\n",
            "Epoch 44/300\n",
            "142/142 [==============================] - 8s 57ms/step - loss: 0.5601 - accuracy: 0.7381 - val_loss: 0.6749 - val_accuracy: 0.6000\n",
            "Epoch 45/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.5581 - accuracy: 0.7337 - val_loss: 0.6801 - val_accuracy: 0.5654\n",
            "Epoch 46/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.5510 - accuracy: 0.7434 - val_loss: 0.6790 - val_accuracy: 0.5692\n",
            "Epoch 47/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.5466 - accuracy: 0.7443 - val_loss: 0.6780 - val_accuracy: 0.5769\n",
            "Epoch 48/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.5384 - accuracy: 0.7403 - val_loss: 0.6835 - val_accuracy: 0.5538\n",
            "Epoch 49/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.5353 - accuracy: 0.7447 - val_loss: 0.6823 - val_accuracy: 0.5731\n",
            "Epoch 50/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.5312 - accuracy: 0.7434 - val_loss: 0.6905 - val_accuracy: 0.5577\n",
            "Epoch 51/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.5234 - accuracy: 0.7702 - val_loss: 0.7088 - val_accuracy: 0.5538\n",
            "Epoch 52/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.5217 - accuracy: 0.7579 - val_loss: 0.6914 - val_accuracy: 0.5692\n",
            "Epoch 53/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.5127 - accuracy: 0.7685 - val_loss: 0.6979 - val_accuracy: 0.5654\n",
            "Epoch 54/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.5001 - accuracy: 0.7628 - val_loss: 0.6953 - val_accuracy: 0.5692\n",
            "Epoch 55/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.5011 - accuracy: 0.7738 - val_loss: 0.7049 - val_accuracy: 0.5692\n",
            "Epoch 56/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.4963 - accuracy: 0.7733 - val_loss: 0.6972 - val_accuracy: 0.5769\n",
            "Epoch 57/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.4965 - accuracy: 0.7720 - val_loss: 0.7055 - val_accuracy: 0.5692\n",
            "Epoch 58/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.4910 - accuracy: 0.7795 - val_loss: 0.7064 - val_accuracy: 0.5654\n",
            "Epoch 59/300\n",
            "142/142 [==============================] - 8s 54ms/step - loss: 0.4860 - accuracy: 0.7773 - val_loss: 0.7129 - val_accuracy: 0.5769\n",
            "Epoch 60/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.4718 - accuracy: 0.7887 - val_loss: 0.7143 - val_accuracy: 0.5577\n",
            "Epoch 61/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.4691 - accuracy: 0.7971 - val_loss: 0.7164 - val_accuracy: 0.5615\n",
            "Epoch 62/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.4758 - accuracy: 0.7879 - val_loss: 0.7247 - val_accuracy: 0.5615\n",
            "Epoch 63/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.4689 - accuracy: 0.7914 - val_loss: 0.7119 - val_accuracy: 0.5808\n",
            "Epoch 64/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.4573 - accuracy: 0.7927 - val_loss: 0.7223 - val_accuracy: 0.5769\n",
            "Epoch 65/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.4564 - accuracy: 0.8107 - val_loss: 0.7203 - val_accuracy: 0.5769\n",
            "Epoch 66/300\n",
            "142/142 [==============================] - 7s 52ms/step - loss: 0.4476 - accuracy: 0.8063 - val_loss: 0.7281 - val_accuracy: 0.5692\n",
            "Epoch 67/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.4509 - accuracy: 0.8059 - val_loss: 0.7272 - val_accuracy: 0.5692\n",
            "Epoch 68/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.4522 - accuracy: 0.8068 - val_loss: 0.7214 - val_accuracy: 0.5923\n",
            "Epoch 69/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.4474 - accuracy: 0.8033 - val_loss: 0.7299 - val_accuracy: 0.5808\n",
            "Epoch 70/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.4293 - accuracy: 0.8178 - val_loss: 0.7462 - val_accuracy: 0.5731\n",
            "Epoch 71/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.4309 - accuracy: 0.8103 - val_loss: 0.7331 - val_accuracy: 0.5885\n",
            "Epoch 72/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.4280 - accuracy: 0.8103 - val_loss: 0.7576 - val_accuracy: 0.5846\n",
            "Epoch 73/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.4206 - accuracy: 0.8182 - val_loss: 0.7452 - val_accuracy: 0.5769\n",
            "Epoch 74/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.4144 - accuracy: 0.8283 - val_loss: 0.7585 - val_accuracy: 0.5808\n",
            "Epoch 75/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.4173 - accuracy: 0.8222 - val_loss: 0.7535 - val_accuracy: 0.5808\n",
            "Epoch 76/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.4043 - accuracy: 0.8402 - val_loss: 0.7708 - val_accuracy: 0.5808\n",
            "Epoch 77/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.3919 - accuracy: 0.8358 - val_loss: 0.7609 - val_accuracy: 0.5885\n",
            "Epoch 78/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.3977 - accuracy: 0.8336 - val_loss: 0.7722 - val_accuracy: 0.5846\n",
            "Epoch 79/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.3934 - accuracy: 0.8314 - val_loss: 0.7657 - val_accuracy: 0.5846\n",
            "Epoch 80/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.3922 - accuracy: 0.8389 - val_loss: 0.7850 - val_accuracy: 0.5846\n",
            "Epoch 81/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.3864 - accuracy: 0.8429 - val_loss: 0.7726 - val_accuracy: 0.5885\n",
            "Epoch 82/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.3908 - accuracy: 0.8371 - val_loss: 0.7835 - val_accuracy: 0.5808\n",
            "Epoch 83/300\n",
            "142/142 [==============================] - 7s 52ms/step - loss: 0.3820 - accuracy: 0.8393 - val_loss: 0.7800 - val_accuracy: 0.5846\n",
            "Epoch 84/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.3648 - accuracy: 0.8578 - val_loss: 0.8036 - val_accuracy: 0.5846\n",
            "Epoch 85/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.3699 - accuracy: 0.8490 - val_loss: 0.8070 - val_accuracy: 0.5846\n",
            "Epoch 86/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.3728 - accuracy: 0.8512 - val_loss: 0.7925 - val_accuracy: 0.5923\n",
            "Epoch 87/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.3649 - accuracy: 0.8508 - val_loss: 0.7938 - val_accuracy: 0.5846\n",
            "Epoch 88/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.3622 - accuracy: 0.8526 - val_loss: 0.8214 - val_accuracy: 0.5885\n",
            "Epoch 89/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.3676 - accuracy: 0.8482 - val_loss: 0.8067 - val_accuracy: 0.5923\n",
            "Epoch 90/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.3468 - accuracy: 0.8653 - val_loss: 0.8137 - val_accuracy: 0.5962\n",
            "Epoch 91/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.3537 - accuracy: 0.8614 - val_loss: 0.8206 - val_accuracy: 0.5885\n",
            "Epoch 92/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.3608 - accuracy: 0.8583 - val_loss: 0.8375 - val_accuracy: 0.5923\n",
            "Epoch 93/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.3497 - accuracy: 0.8640 - val_loss: 0.8368 - val_accuracy: 0.5923\n",
            "Epoch 94/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.3305 - accuracy: 0.8697 - val_loss: 0.8477 - val_accuracy: 0.6000\n",
            "Epoch 95/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.3452 - accuracy: 0.8609 - val_loss: 0.8301 - val_accuracy: 0.5962\n",
            "Epoch 96/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.3364 - accuracy: 0.8658 - val_loss: 0.8486 - val_accuracy: 0.5923\n",
            "Epoch 97/300\n",
            "142/142 [==============================] - 7s 53ms/step - loss: 0.3308 - accuracy: 0.8781 - val_loss: 0.8493 - val_accuracy: 0.6000\n",
            "Epoch 98/300\n",
            "142/142 [==============================] - 8s 56ms/step - loss: 0.3305 - accuracy: 0.8658 - val_loss: 0.8416 - val_accuracy: 0.6154\n",
            "Epoch 99/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.3244 - accuracy: 0.8763 - val_loss: 0.8478 - val_accuracy: 0.6000\n",
            "Epoch 100/300\n",
            "142/142 [==============================] - 8s 54ms/step - loss: 0.3241 - accuracy: 0.8702 - val_loss: 0.8671 - val_accuracy: 0.6000\n",
            "Epoch 101/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.3178 - accuracy: 0.8728 - val_loss: 0.8689 - val_accuracy: 0.6038\n",
            "Epoch 102/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.3121 - accuracy: 0.8794 - val_loss: 0.9007 - val_accuracy: 0.5962\n",
            "Epoch 103/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.3120 - accuracy: 0.8781 - val_loss: 0.8760 - val_accuracy: 0.6115\n",
            "Epoch 104/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.3108 - accuracy: 0.8834 - val_loss: 0.8845 - val_accuracy: 0.6154\n",
            "Epoch 105/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.3026 - accuracy: 0.8886 - val_loss: 0.8760 - val_accuracy: 0.6038\n",
            "Epoch 106/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.2920 - accuracy: 0.8913 - val_loss: 0.8885 - val_accuracy: 0.6038\n",
            "Epoch 107/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.2934 - accuracy: 0.8869 - val_loss: 0.8938 - val_accuracy: 0.6154\n",
            "Epoch 108/300\n",
            "142/142 [==============================] - 8s 57ms/step - loss: 0.2932 - accuracy: 0.8882 - val_loss: 0.8945 - val_accuracy: 0.6192\n",
            "Epoch 109/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.2931 - accuracy: 0.8908 - val_loss: 0.9015 - val_accuracy: 0.6115\n",
            "Epoch 110/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.2864 - accuracy: 0.8939 - val_loss: 0.9370 - val_accuracy: 0.6000\n",
            "Epoch 111/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.2921 - accuracy: 0.8869 - val_loss: 0.9432 - val_accuracy: 0.6115\n",
            "Epoch 112/300\n",
            "142/142 [==============================] - 8s 57ms/step - loss: 0.2784 - accuracy: 0.8952 - val_loss: 0.9082 - val_accuracy: 0.6308\n",
            "Epoch 113/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.2792 - accuracy: 0.8983 - val_loss: 0.9154 - val_accuracy: 0.6231\n",
            "Epoch 114/300\n",
            "142/142 [==============================] - 8s 57ms/step - loss: 0.2790 - accuracy: 0.8979 - val_loss: 0.9170 - val_accuracy: 0.6346\n",
            "Epoch 115/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.2757 - accuracy: 0.8988 - val_loss: 0.9331 - val_accuracy: 0.6192\n",
            "Epoch 116/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.2781 - accuracy: 0.8922 - val_loss: 0.9716 - val_accuracy: 0.6038\n",
            "Epoch 117/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.2731 - accuracy: 0.8974 - val_loss: 0.9478 - val_accuracy: 0.6269\n",
            "Epoch 118/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.2626 - accuracy: 0.9032 - val_loss: 0.9593 - val_accuracy: 0.6308\n",
            "Epoch 119/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.2547 - accuracy: 0.9027 - val_loss: 0.9564 - val_accuracy: 0.6231\n",
            "Epoch 120/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.2581 - accuracy: 0.9076 - val_loss: 0.9463 - val_accuracy: 0.6269\n",
            "Epoch 121/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.2580 - accuracy: 0.9018 - val_loss: 0.9704 - val_accuracy: 0.6308\n",
            "Epoch 122/300\n",
            "142/142 [==============================] - 8s 57ms/step - loss: 0.2509 - accuracy: 0.9040 - val_loss: 0.9744 - val_accuracy: 0.6385\n",
            "Epoch 123/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.2564 - accuracy: 0.9058 - val_loss: 0.9808 - val_accuracy: 0.6385\n",
            "Epoch 124/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.2606 - accuracy: 0.9010 - val_loss: 0.9674 - val_accuracy: 0.6346\n",
            "Epoch 125/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.2455 - accuracy: 0.9098 - val_loss: 0.9945 - val_accuracy: 0.6385\n",
            "Epoch 126/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.2399 - accuracy: 0.9181 - val_loss: 0.9614 - val_accuracy: 0.6308\n",
            "Epoch 127/300\n",
            "142/142 [==============================] - 8s 57ms/step - loss: 0.2314 - accuracy: 0.9195 - val_loss: 0.9996 - val_accuracy: 0.6462\n",
            "Epoch 128/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.2343 - accuracy: 0.9181 - val_loss: 1.0070 - val_accuracy: 0.6423\n",
            "Epoch 129/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.2374 - accuracy: 0.9124 - val_loss: 0.9950 - val_accuracy: 0.6462\n",
            "Epoch 130/300\n",
            "142/142 [==============================] - 8s 54ms/step - loss: 0.2252 - accuracy: 0.9151 - val_loss: 1.0342 - val_accuracy: 0.6269\n",
            "Epoch 131/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.2330 - accuracy: 0.9155 - val_loss: 0.9883 - val_accuracy: 0.6423\n",
            "Epoch 132/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.2347 - accuracy: 0.9120 - val_loss: 1.0447 - val_accuracy: 0.6423\n",
            "Epoch 133/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.2280 - accuracy: 0.9234 - val_loss: 1.0070 - val_accuracy: 0.6385\n",
            "Epoch 134/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.2286 - accuracy: 0.9190 - val_loss: 1.0113 - val_accuracy: 0.6462\n",
            "Epoch 135/300\n",
            "142/142 [==============================] - 8s 57ms/step - loss: 0.2192 - accuracy: 0.9256 - val_loss: 1.0404 - val_accuracy: 0.6577\n",
            "Epoch 136/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.2200 - accuracy: 0.9212 - val_loss: 1.0327 - val_accuracy: 0.6500\n",
            "Epoch 137/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.2084 - accuracy: 0.9313 - val_loss: 1.0254 - val_accuracy: 0.6346\n",
            "Epoch 138/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.2142 - accuracy: 0.9256 - val_loss: 1.0552 - val_accuracy: 0.6577\n",
            "Epoch 139/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.2149 - accuracy: 0.9234 - val_loss: 1.0527 - val_accuracy: 0.6423\n",
            "Epoch 140/300\n",
            "142/142 [==============================] - 8s 54ms/step - loss: 0.2091 - accuracy: 0.9283 - val_loss: 1.0684 - val_accuracy: 0.6423\n",
            "Epoch 141/300\n",
            "142/142 [==============================] - 8s 54ms/step - loss: 0.1970 - accuracy: 0.9371 - val_loss: 1.0645 - val_accuracy: 0.6423\n",
            "Epoch 142/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.2008 - accuracy: 0.9313 - val_loss: 1.0829 - val_accuracy: 0.6577\n",
            "Epoch 143/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.2008 - accuracy: 0.9318 - val_loss: 1.0697 - val_accuracy: 0.6346\n",
            "Epoch 144/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1964 - accuracy: 0.9331 - val_loss: 1.0713 - val_accuracy: 0.6423\n",
            "Epoch 145/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1962 - accuracy: 0.9340 - val_loss: 1.1115 - val_accuracy: 0.6577\n",
            "Epoch 146/300\n",
            "142/142 [==============================] - 8s 54ms/step - loss: 0.1907 - accuracy: 0.9265 - val_loss: 1.0771 - val_accuracy: 0.6500\n",
            "Epoch 147/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1879 - accuracy: 0.9283 - val_loss: 1.0653 - val_accuracy: 0.6423\n",
            "Epoch 148/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1952 - accuracy: 0.9362 - val_loss: 1.0969 - val_accuracy: 0.6500\n",
            "Epoch 149/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1859 - accuracy: 0.9375 - val_loss: 1.1388 - val_accuracy: 0.6538\n",
            "Epoch 150/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1839 - accuracy: 0.9349 - val_loss: 1.1381 - val_accuracy: 0.6577\n",
            "Epoch 151/300\n",
            "142/142 [==============================] - 8s 57ms/step - loss: 0.1847 - accuracy: 0.9362 - val_loss: 1.1395 - val_accuracy: 0.6615\n",
            "Epoch 152/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1841 - accuracy: 0.9371 - val_loss: 1.1911 - val_accuracy: 0.6385\n",
            "Epoch 153/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1740 - accuracy: 0.9459 - val_loss: 1.1451 - val_accuracy: 0.6577\n",
            "Epoch 154/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1866 - accuracy: 0.9340 - val_loss: 1.1420 - val_accuracy: 0.6500\n",
            "Epoch 155/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1705 - accuracy: 0.9476 - val_loss: 1.1356 - val_accuracy: 0.6385\n",
            "Epoch 156/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1751 - accuracy: 0.9410 - val_loss: 1.1262 - val_accuracy: 0.6423\n",
            "Epoch 157/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1839 - accuracy: 0.9375 - val_loss: 1.1354 - val_accuracy: 0.6577\n",
            "Epoch 158/300\n",
            "142/142 [==============================] - 8s 57ms/step - loss: 0.1762 - accuracy: 0.9419 - val_loss: 1.1704 - val_accuracy: 0.6654\n",
            "Epoch 159/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1700 - accuracy: 0.9445 - val_loss: 1.1864 - val_accuracy: 0.6577\n",
            "Epoch 160/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1658 - accuracy: 0.9423 - val_loss: 1.1471 - val_accuracy: 0.6423\n",
            "Epoch 161/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1629 - accuracy: 0.9459 - val_loss: 1.2297 - val_accuracy: 0.6462\n",
            "Epoch 162/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1607 - accuracy: 0.9441 - val_loss: 1.2144 - val_accuracy: 0.6538\n",
            "Epoch 163/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1583 - accuracy: 0.9507 - val_loss: 1.2273 - val_accuracy: 0.6538\n",
            "Epoch 164/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1605 - accuracy: 0.9423 - val_loss: 1.1805 - val_accuracy: 0.6538\n",
            "Epoch 165/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1544 - accuracy: 0.9472 - val_loss: 1.2427 - val_accuracy: 0.6500\n",
            "Epoch 166/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1569 - accuracy: 0.9445 - val_loss: 1.2029 - val_accuracy: 0.6538\n",
            "Epoch 167/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1615 - accuracy: 0.9445 - val_loss: 1.2267 - val_accuracy: 0.6462\n",
            "Epoch 168/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1556 - accuracy: 0.9511 - val_loss: 1.1794 - val_accuracy: 0.6308\n",
            "Epoch 169/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1542 - accuracy: 0.9520 - val_loss: 1.2293 - val_accuracy: 0.6577\n",
            "Epoch 170/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1459 - accuracy: 0.9547 - val_loss: 1.2443 - val_accuracy: 0.6577\n",
            "Epoch 171/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1556 - accuracy: 0.9481 - val_loss: 1.2242 - val_accuracy: 0.6462\n",
            "Epoch 172/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1456 - accuracy: 0.9586 - val_loss: 1.2751 - val_accuracy: 0.6500\n",
            "Epoch 173/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1425 - accuracy: 0.9511 - val_loss: 1.2793 - val_accuracy: 0.6538\n",
            "Epoch 174/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1434 - accuracy: 0.9542 - val_loss: 1.2524 - val_accuracy: 0.6462\n",
            "Epoch 175/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1456 - accuracy: 0.9507 - val_loss: 1.2766 - val_accuracy: 0.6577\n",
            "Epoch 176/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1430 - accuracy: 0.9577 - val_loss: 1.3000 - val_accuracy: 0.6615\n",
            "Epoch 177/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1380 - accuracy: 0.9560 - val_loss: 1.2834 - val_accuracy: 0.6615\n",
            "Epoch 178/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1421 - accuracy: 0.9547 - val_loss: 1.3126 - val_accuracy: 0.6500\n",
            "Epoch 179/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1321 - accuracy: 0.9573 - val_loss: 1.2732 - val_accuracy: 0.6462\n",
            "Epoch 180/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1420 - accuracy: 0.9551 - val_loss: 1.2888 - val_accuracy: 0.6500\n",
            "Epoch 181/300\n",
            "142/142 [==============================] - 8s 54ms/step - loss: 0.1277 - accuracy: 0.9599 - val_loss: 1.3128 - val_accuracy: 0.6462\n",
            "Epoch 182/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1337 - accuracy: 0.9626 - val_loss: 1.2921 - val_accuracy: 0.6500\n",
            "Epoch 183/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1284 - accuracy: 0.9613 - val_loss: 1.3183 - val_accuracy: 0.6462\n",
            "Epoch 184/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1314 - accuracy: 0.9617 - val_loss: 1.3656 - val_accuracy: 0.6500\n",
            "Epoch 185/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1256 - accuracy: 0.9652 - val_loss: 1.3160 - val_accuracy: 0.6500\n",
            "Epoch 186/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1237 - accuracy: 0.9635 - val_loss: 1.3371 - val_accuracy: 0.6462\n",
            "Epoch 187/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1340 - accuracy: 0.9595 - val_loss: 1.3025 - val_accuracy: 0.6462\n",
            "Epoch 188/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1293 - accuracy: 0.9573 - val_loss: 1.3731 - val_accuracy: 0.6423\n",
            "Epoch 189/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1278 - accuracy: 0.9564 - val_loss: 1.3334 - val_accuracy: 0.6577\n",
            "Epoch 190/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1158 - accuracy: 0.9736 - val_loss: 1.3720 - val_accuracy: 0.6423\n",
            "Epoch 191/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1211 - accuracy: 0.9657 - val_loss: 1.3562 - val_accuracy: 0.6423\n",
            "Epoch 192/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1222 - accuracy: 0.9595 - val_loss: 1.3673 - val_accuracy: 0.6462\n",
            "Epoch 193/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1176 - accuracy: 0.9635 - val_loss: 1.3619 - val_accuracy: 0.6462\n",
            "Epoch 194/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1138 - accuracy: 0.9621 - val_loss: 1.3696 - val_accuracy: 0.6538\n",
            "Epoch 195/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1163 - accuracy: 0.9630 - val_loss: 1.3746 - val_accuracy: 0.6538\n",
            "Epoch 196/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1142 - accuracy: 0.9679 - val_loss: 1.3522 - val_accuracy: 0.6500\n",
            "Epoch 197/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1184 - accuracy: 0.9635 - val_loss: 1.3931 - val_accuracy: 0.6462\n",
            "Epoch 198/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1126 - accuracy: 0.9683 - val_loss: 1.3942 - val_accuracy: 0.6500\n",
            "Epoch 199/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1150 - accuracy: 0.9639 - val_loss: 1.3947 - val_accuracy: 0.6500\n",
            "Epoch 200/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1097 - accuracy: 0.9705 - val_loss: 1.3465 - val_accuracy: 0.6538\n",
            "Epoch 201/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1124 - accuracy: 0.9674 - val_loss: 1.3990 - val_accuracy: 0.6538\n",
            "Epoch 202/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1054 - accuracy: 0.9692 - val_loss: 1.4455 - val_accuracy: 0.6500\n",
            "Epoch 203/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1129 - accuracy: 0.9648 - val_loss: 1.4064 - val_accuracy: 0.6500\n",
            "Epoch 204/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0989 - accuracy: 0.9705 - val_loss: 1.4267 - val_accuracy: 0.6500\n",
            "Epoch 205/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1019 - accuracy: 0.9710 - val_loss: 1.4381 - val_accuracy: 0.6538\n",
            "Epoch 206/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1033 - accuracy: 0.9683 - val_loss: 1.4540 - val_accuracy: 0.6538\n",
            "Epoch 207/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0963 - accuracy: 0.9749 - val_loss: 1.4463 - val_accuracy: 0.6500\n",
            "Epoch 208/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0944 - accuracy: 0.9696 - val_loss: 1.4334 - val_accuracy: 0.6538\n",
            "Epoch 209/300\n",
            "142/142 [==============================] - 8s 54ms/step - loss: 0.1012 - accuracy: 0.9710 - val_loss: 1.4419 - val_accuracy: 0.6500\n",
            "Epoch 210/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1036 - accuracy: 0.9652 - val_loss: 1.4254 - val_accuracy: 0.6500\n",
            "Epoch 211/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0995 - accuracy: 0.9710 - val_loss: 1.4733 - val_accuracy: 0.6538\n",
            "Epoch 212/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0968 - accuracy: 0.9692 - val_loss: 1.4094 - val_accuracy: 0.6308\n",
            "Epoch 213/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0961 - accuracy: 0.9732 - val_loss: 1.4860 - val_accuracy: 0.6538\n",
            "Epoch 214/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0936 - accuracy: 0.9745 - val_loss: 1.4622 - val_accuracy: 0.6500\n",
            "Epoch 215/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0960 - accuracy: 0.9701 - val_loss: 1.4939 - val_accuracy: 0.6615\n",
            "Epoch 216/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0917 - accuracy: 0.9723 - val_loss: 1.4744 - val_accuracy: 0.6500\n",
            "Epoch 217/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.1041 - accuracy: 0.9688 - val_loss: 1.5211 - val_accuracy: 0.6577\n",
            "Epoch 218/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0999 - accuracy: 0.9701 - val_loss: 1.4439 - val_accuracy: 0.6500\n",
            "Epoch 219/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0897 - accuracy: 0.9758 - val_loss: 1.4692 - val_accuracy: 0.6500\n",
            "Epoch 220/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0911 - accuracy: 0.9771 - val_loss: 1.4646 - val_accuracy: 0.6538\n",
            "Epoch 221/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0865 - accuracy: 0.9780 - val_loss: 1.4500 - val_accuracy: 0.6385\n",
            "Epoch 222/300\n",
            "142/142 [==============================] - 8s 54ms/step - loss: 0.0865 - accuracy: 0.9776 - val_loss: 1.4443 - val_accuracy: 0.6462\n",
            "Epoch 223/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0849 - accuracy: 0.9749 - val_loss: 1.5204 - val_accuracy: 0.6538\n",
            "Epoch 224/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0931 - accuracy: 0.9749 - val_loss: 1.4720 - val_accuracy: 0.6423\n",
            "Epoch 225/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0844 - accuracy: 0.9776 - val_loss: 1.4959 - val_accuracy: 0.6462\n",
            "Epoch 226/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0812 - accuracy: 0.9798 - val_loss: 1.5597 - val_accuracy: 0.6500\n",
            "Epoch 227/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0783 - accuracy: 0.9806 - val_loss: 1.4890 - val_accuracy: 0.6462\n",
            "Epoch 228/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0762 - accuracy: 0.9793 - val_loss: 1.4846 - val_accuracy: 0.6423\n",
            "Epoch 229/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0756 - accuracy: 0.9824 - val_loss: 1.5729 - val_accuracy: 0.6538\n",
            "Epoch 230/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0766 - accuracy: 0.9811 - val_loss: 1.5100 - val_accuracy: 0.6423\n",
            "Epoch 231/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0783 - accuracy: 0.9802 - val_loss: 1.5104 - val_accuracy: 0.6423\n",
            "Epoch 232/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0839 - accuracy: 0.9732 - val_loss: 1.4744 - val_accuracy: 0.6423\n",
            "Epoch 233/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0819 - accuracy: 0.9762 - val_loss: 1.5565 - val_accuracy: 0.6500\n",
            "Epoch 234/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0686 - accuracy: 0.9855 - val_loss: 1.5378 - val_accuracy: 0.6385\n",
            "Epoch 235/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0738 - accuracy: 0.9798 - val_loss: 1.5891 - val_accuracy: 0.6538\n",
            "Epoch 236/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0721 - accuracy: 0.9828 - val_loss: 1.5707 - val_accuracy: 0.6500\n",
            "Epoch 237/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0733 - accuracy: 0.9784 - val_loss: 1.5791 - val_accuracy: 0.6500\n",
            "Epoch 238/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0713 - accuracy: 0.9798 - val_loss: 1.5907 - val_accuracy: 0.6462\n",
            "Epoch 239/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0716 - accuracy: 0.9828 - val_loss: 1.6328 - val_accuracy: 0.6462\n",
            "Epoch 240/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0654 - accuracy: 0.9846 - val_loss: 1.5909 - val_accuracy: 0.6462\n",
            "Epoch 241/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0682 - accuracy: 0.9811 - val_loss: 1.6259 - val_accuracy: 0.6500\n",
            "Epoch 242/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0707 - accuracy: 0.9820 - val_loss: 1.5726 - val_accuracy: 0.6462\n",
            "Epoch 243/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0692 - accuracy: 0.9837 - val_loss: 1.6127 - val_accuracy: 0.6462\n",
            "Epoch 244/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0659 - accuracy: 0.9828 - val_loss: 1.5897 - val_accuracy: 0.6500\n",
            "Epoch 245/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0624 - accuracy: 0.9833 - val_loss: 1.6488 - val_accuracy: 0.6538\n",
            "Epoch 246/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0659 - accuracy: 0.9828 - val_loss: 1.6883 - val_accuracy: 0.6538\n",
            "Epoch 247/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0697 - accuracy: 0.9789 - val_loss: 1.6594 - val_accuracy: 0.6538\n",
            "Epoch 248/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0631 - accuracy: 0.9842 - val_loss: 1.5877 - val_accuracy: 0.6385\n",
            "Epoch 249/300\n",
            "142/142 [==============================] - 8s 53ms/step - loss: 0.0650 - accuracy: 0.9859 - val_loss: 1.6569 - val_accuracy: 0.6500\n",
            "Epoch 250/300\n",
            " 97/142 [===================>..........] - ETA: 2s - loss: 0.0638 - accuracy: 0.9807Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz0yJ07AUQve",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bb4fd3b-e3c8-437a-f66b-6971c9ee1ba1"
      },
      "source": [
        "sum(maxes)/len(maxes), sum(ensemblePrediction)/len(ensemblePrediction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6784383356571198, 0.6359838654546981)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Cvcu4cpTLmq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faa7750b-2656-48ee-c20a-85975491cbb3"
      },
      "source": [
        "statistics.stdev(maxes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.06359719809292419"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z23HVMI8TYSZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db6b9e65-bdee-47b0-a5d6-254ca0323aa4"
      },
      "source": [
        "statistics.stdev(ensemblePrediction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3913295301539358"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rTdgxg9Ta_v"
      },
      "source": [
        "  # print(model.predict(x_test))\n",
        "  # predictions = np.argmax(model.predict(x_test))\n",
        "  predictions = model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vtyQMhMbn7s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1214a3f-42e4-49d9-f457-ccbc2ccf5aff"
      },
      "source": [
        "for i in predictions:\n",
        "  print(np.argmax(i[0][0]))\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIp_FKZ7bpIL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e83998c-580e-4d3d-dc2c-72b7d446d701"
      },
      "source": [
        "ensembleMax"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.5,\n",
              " 3.75,\n",
              " 8.5,\n",
              " 2.1666666666666665,\n",
              " 2.1666666666666665,\n",
              " 3.75,\n",
              " 3.75,\n",
              " 2.8,\n",
              " 1.375,\n",
              " 2.1666666666666665]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5zi_MG0bvKB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22aeaca2-faa6-4500-f0e7-8f880de00b6e"
      },
      "source": [
        "ensemblePrediction"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8666666666666667,\n",
              " 0.0,\n",
              " 0.7333333333333333,\n",
              " 0.875,\n",
              " 0.1111111111111111,\n",
              " 0.9285714285714286,\n",
              " 1.0,\n",
              " 0.8571428571428571,\n",
              " 0.15384615384615385,\n",
              " 0.8571428571428571,\n",
              " 0.9375,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.3684210526315789,\n",
              " 0.8235294117647058,\n",
              " 0.5714285714285714,\n",
              " 1.0,\n",
              " 1.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jISl7SkmKIAL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}