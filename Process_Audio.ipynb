{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Process Audio.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNJQ0EPyjTaKC1jNYm7Dknv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atjessehill/Thesis-Notebooks/blob/main/Process_Audio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWZpImNLwMtJ",
        "outputId": "c2ffabfb-1059-4d1f-c1d3-fabc96e4e406",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "from os import path\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import json\n",
        "from sklearn import preprocessing\n",
        "import glob\n",
        "drive.mount('/content/drive')\n",
        "from skimage.io import imsave, imread\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCv9dFkFbDyO"
      },
      "source": [
        "\"\"\"\n",
        "A collection of functions and snippets for extracting audio features\n",
        "1. MFCC feature extraction adapted from \"The Sound of AI\" Youtube Channel\n",
        "2. Extracting and saving melspectrogram, and slicing songs into 30 second clips\n",
        "   before extracting melspectrogram\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQk3NYW4oxoY"
      },
      "source": [
        "BASE = 'drive/My Drive'\n",
        "SONG_SAMPLE_PATH = 'Thesis/Samples'\n",
        "DATA_SAVE_PATH = 'Thesis/InputData'\n",
        "SONG_FULL_PATH = 'Thesis/Samples/FULL_TRACKS'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88L0gP3qwzDz"
      },
      "source": [
        "def rescale_feature(x):\n",
        "  scaler = preprocessing.MinMaxScaler()\n",
        "  scaler.fit(x)\n",
        "\n",
        "  return scaler.transform(x)\n",
        "\n",
        "def load_splits():\n",
        "  with open(os.path.join(BASE, DATA_SAVE_PATH, '10-fold-splits.json'), 'r') as fp:\n",
        "    data = json.load(fp)\n",
        "\n",
        "  return data\n",
        "\n",
        "def scale_minmax(X, min=0.0, max=1.0):\n",
        "  X_std = (X - X.min()) / (X.max() - X.min())\n",
        "  X_scaled = X_std * (max - min) + min\n",
        "  return X_scaled\n",
        "\n",
        "def save_mfcc(json_file, n_mfcc=20, n_fft=2048, hop_length=512, rescale=False, debug=False):\n",
        "\n",
        "  json_save_file = os.path.join(BASE, DATA_SAVE_PATH, json_file)\n",
        "\n",
        "  data = {\n",
        "      'data': 13,\n",
        "\n",
        "            'mapping': {\n",
        "          'noDJ': 0,\n",
        "          'yesDJ': 1\n",
        "      },\n",
        "      'mfcc': [],\n",
        "      'labels': []\n",
        "  }\n",
        "\n",
        "  paths = ['noDJ', 'yesDJ']\n",
        "  shape = None\n",
        "  for i, (p) in enumerate(paths):\n",
        "    for j, (file) in enumerate(glob.glob(os.path.join(BASE, SONG_SAMPLE_PATH, p, \"*.mp3\"))):\n",
        "      if j % 25 == 0:\n",
        "        print(\"j{} i{} Loading{}\".format(j, i, file))\n",
        "      #print(\"j{} i{} Loading{}\".format(j, i, file))\n",
        "      signal, sr = librosa.load(file, sr=22050)\n",
        "      mfcc = librosa.feature.mfcc(signal, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
        "      mfcc = mfcc.T\n",
        "      if rescale:\n",
        "        mfcc = rescale_feature(mfcc)\n",
        "      if j == 0:\n",
        "        print(\"Shape is {}\".format(mfcc.shape))\n",
        "        shape = mfcc.shape\n",
        "      if mfcc.shape == shape:\n",
        "\n",
        "        data['mfcc'].append(mfcc.tolist())\n",
        "        data['labels'].append(i)\n",
        "\n",
        "      else:\n",
        "        print(\"Coudln't save {} with shape {} on file {}\".format(file, mfcc.shape, file.split('//')[-1]))\n",
        "\n",
        "      if debug and j==2:\n",
        "        break\n",
        "    if debug:\n",
        "      return data\n",
        "\n",
        "    with open(json_save_file, 'w') as fp:\n",
        "      json.dump(data, fp, indent=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DB6MAAAC1ZZX"
      },
      "source": [
        "def load_data(path):\n",
        "  with open(os.path.join(BASE, DATA_SAVE_PATH, 'All', path), 'r') as fp:\n",
        "    data = json.load(fp)\n",
        "\n",
        "  x = np.array(data['mfcc'])\n",
        "  y = np.array(data['labels'])\n",
        "  return x, y\n",
        "\n",
        "def load_data_list(path):\n",
        "  with open(os.path.join(BASE, DATA_SAVE_PATH, path), 'r') as fp:\n",
        "    data = json.load(fp)\n",
        "\n",
        "  x = data['mfcc']\n",
        "  y = data['labels']\n",
        "  return x, y\n",
        "\n",
        "def setup_mfcc():\n",
        "\n",
        "  n_mfcc = [20]\n",
        "  # #n_mfcc = [13, 14, 15, 16]\n",
        "  n_fft = [1024]\n",
        "  hop_length = [1024]\n",
        "\n",
        "  for nmfcc in n_mfcc:\n",
        "    for nfft in n_fft:\n",
        "      for hoplength in hop_length:\n",
        "        json_file = \"MFCC_{}_nfft_{}_hl_{}.json\".format(nmfcc, nfft, hoplength)\n",
        "        if nfft != hoplength:\n",
        "          print(f\"Skipping {json_file}\")\n",
        "          # continue\n",
        "        else:\n",
        "          print(\"Calling {}\".format(json_file))\n",
        "          save_mfcc(json_file, n_mfcc=nmfcc, n_fft=nfft, hop_length=hoplength, rescale=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v36rLpmgBwnJ"
      },
      "source": [
        "# Function to remove MFCC\n",
        "\n",
        "def twenty_to_fewer_mfcc():\n",
        "\n",
        "  n_mfcc = [13, 14, 15, 16, 17, 18, 19]\n",
        "  n_fft = [1024]\n",
        "  hop_length = [1024]\n",
        "  for nfft in n_fft:\n",
        "    for hoplength in hop_length:\n",
        "      data = {\n",
        "          'mapping': {\n",
        "              'noDJ': 0,\n",
        "              'yesDJ': 1\n",
        "          },\n",
        "          'mfcc': [],\n",
        "          'labels': []\n",
        "      }\n",
        "\n",
        "      if nfft == 512 and hoplength == 512:\n",
        "        continue\n",
        "      \n",
        "      twenty_ref = f'MFCC_20_nfft_{nfft}_hl_{hoplength}.json'\n",
        "      if path.exists(os.path.join(BASE, DATA_SAVE_PATH, twenty_ref)):\n",
        "        print(f\"Loading {twenty_ref}\")\n",
        "        x, y = load_data_list(twenty_ref)      \n",
        "      \n",
        "      for nmfcc in n_mfcc:\n",
        "        json_save_file = os.path.join(BASE, DATA_SAVE_PATH, f'MFCC_{nmfcc}_nfft_{nfft}_hl_{hoplength}.json')\n",
        "        outer = []\n",
        "        for i in x:\n",
        "          inner = []\n",
        "          for j in i:\n",
        "            inner.append(j[:nmfcc])\n",
        "          outer.append(inner)\n",
        "\n",
        "        data['mfcc'] = outer\n",
        "        data['labels'] = y\n",
        "\n",
        "        with open(json_save_file, 'w') as fp:\n",
        "          json.dump(data, fp, indent=4)\n",
        "          print(\"Created \"+json_save_file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FFISsuepG4X"
      },
      "source": [
        "# Extract and save melspectrogram for each file encountered in the filepath, \n",
        "# for each combination of window size and hop length\n",
        "n_fft = [1024]\n",
        "hop_length = [512]\n",
        "paths = ['noDJ', 'yesDJ']\n",
        "for nfft in n_fft:\n",
        "  for hl in hop_length:\n",
        "    print(f\"Starting {nfft} and {hl}\")\n",
        "    # for i, (p) in enumerate(paths):\n",
        "    for j, (file) in enumerate(glob.glob(os.path.join(BASE, SONG_SAMPLE_PATH, \"30_sec\", \"*.mp3\"))):\n",
        "      url = file.split('/')[-1]\n",
        "      url, clip, _ = url.split('.mp3')\n",
        "      out = url+'_spec'+clip+'.npy'\n",
        "      if j % 25 == 0:\n",
        "        print(\"j{} i{} Loading{}\".format(j, i, file))\n",
        "      file_output = os.path.join(BASE, DATA_SAVE_PATH, \"Spectrograms_30sec\", out)\n",
        "      if os.path.exists(file_output):\n",
        "        continue\n",
        "      y, sr = librosa.load(file, sr=22050)\n",
        "      s = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=nfft, hop_length=hl)\n",
        "      mels = np.log(s + 1e-9)\n",
        "      img = scale_minmax(mels, 0.0, 255.0).astype(np.uint8)\n",
        "      img = np.flip(img, axis=0)\n",
        "      img = 255-img\n",
        "      np.save(file_output, img)\n",
        "      if j % 25 == 0:\n",
        "        print(f\"Finished {j}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wLmnFsasR6X"
      },
      "source": [
        "# Extract and save MFCC's for each 3:30 second clip, with different combinations\n",
        "# of MFCC's, window size, and hop length\n",
        "\n",
        "n_mfcc = [20]\n",
        "n_fft = [1024]\n",
        "hop_length = [128, 256, 512, 1024]\n",
        "paths = ['noDJ', 'yesDJ']\n",
        "rescale = True\n",
        "for nmfcc in n_mfcc:\n",
        "  for nfft in n_fft:\n",
        "    for hl in hop_length:\n",
        "      print(f\"Starting {nfft} and {hl}\")\n",
        "      for i, (p) in enumerate(paths):\n",
        "          for j, (file) in enumerate(glob.glob(os.path.join(BASE, SONG_SAMPLE_PATH, p, \"*.mp3\"))):\n",
        "            url = file.split('/')[-1]\n",
        "            out = url+'_MFCC_'+str(nmfcc)+'_'+str(nfft)+'_'+str(hl)+'.npy'\n",
        "            file_output = os.path.join(BASE, DATA_SAVE_PATH, \"MFCC\", out)\n",
        "            if j % 25 == 0:\n",
        "              print(\"j{} i{} Loading{}\".format(j, i, file))\n",
        "            signal, sr = librosa.load(file, sr=22050)\n",
        "\n",
        "            mfcc = librosa.feature.mfcc(signal, sr=sr, n_mfcc=nmfcc, n_fft=nfft, hop_length=hl)\n",
        "            mfcc = mfcc.T\n",
        "            if rescale:\n",
        "              mfcc = rescale_feature(mfcc)\n",
        "            if j == 0:\n",
        "              print(\"Shape is {}\".format(mfcc.shape))\n",
        "              shape = mfcc.shape\n",
        "            if mfcc.shape == shape:\n",
        "              np.save(file_output, mfcc)\n",
        "            else:\n",
        "              print(\"Could not save\", file_output)\n",
        "            print(j)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoU6fCKbq4L8"
      },
      "source": [
        "# Slice each song into 30 second chunks, ignoring the end\n",
        "\n",
        "def slice_audio():\n",
        "\n",
        "  paths = ['noDJ', 'yesDJ']\n",
        "\n",
        "  for i, (p) in enumerate(paths):\n",
        "    for j, (file) in enumerate(glob.glob(os.path.join(BASE, 'Thesis', 'Samples', 'FULL_TRACKS', p, \"*.mp3\"))):\n",
        "      if j % 25 == 0:\n",
        "        print(\"Starting \", i, j, file)\n",
        "\n",
        "      mp3_name = file.split('/')[-1]\n",
        "      song = AudioSegment.from_mp3(file)\n",
        "      offset = 0\n",
        "      remove = len(song) % (30*1000)\n",
        "      song = song[:len(song)-remove]\n",
        "      slices = int(len(song)/(30*1000))\n",
        "      for s in range(0, slices):\n",
        "        start = s*30*1000\n",
        "        end = (s+1)*30*1000\n",
        "        extract = song[start:end]\n",
        "        if len(extract) != 30000:\n",
        "          print(f\"Errored at {file} clip {s} --> {len(extract)}\")\n",
        "        else:\n",
        "          file_name = mp3_name+f'_clip_{s}.mp3'\n",
        "          file_name = os.path.join(BASE, 'Thesis', 'Samples', '30_sec', file_name)\n",
        "          print(file_name)\n",
        "          extract.export(file_name, format='mp3')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_HjyjyirC6P"
      },
      "source": [
        "# Create a dictionary for the dataset which tracks which songs are in which\n",
        "# 10-fold\n",
        "\n",
        "url_splits = []\n",
        "for i, shuffle_index in enumerate(splits):\n",
        "  fold_data = {}\n",
        "  x_train = np.delete(urls_np, shuffle_index)\n",
        "  x_test = np.take(urls_np, shuffle_index)\n",
        "  y_train = np.delete(result_np, shuffle_index)\n",
        "  y_test = np.take(result_np, shuffle_index)\n",
        "\n",
        "  fold_data['train'] = x_train.tolist()\n",
        "  fold_data['test'] = x_test.tolist()\n",
        "  fold_data['ytrain'] = y_train.tolist()\n",
        "  fold_data['ytest'] = y_test.tolist()\n",
        "  url_splits.append(fold_data)\n",
        "\n",
        "\n",
        "json_save_file = os.path.join(BASE, DATA_SAVE_PATH, '10-fold-splits.json')\n",
        "\n",
        "with open(json_save_file, 'w') as fp:\n",
        "  json.dump(url_splits, fp, indent=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbIygfH5UERb"
      },
      "source": [
        "# Old essentia code, TODO: reimport essentia\n",
        "\n",
        "def get_low_level_features(json_file):\n",
        "  json_save_file = os.path.join(BASE, DATA_SAVE_PATH, json_file)\n",
        "\n",
        "  data = {\n",
        "\n",
        "      'mapping': {\n",
        "        'noDJ': 0,\n",
        "        'yesDJ': 1\n",
        "      },\n",
        "      'name': [],\n",
        "      'labels': []\n",
        "  }\n",
        "\n",
        "  paths = ['noDJ', 'yesDJ']\n",
        "  for i, (p) in enumerate(paths):\n",
        "    for j, (file) in enumerate(glob.glob(os.path.join(BASE, SONG_FULL_PATH, p, \"*.mp3\"))):\n",
        "      features, feature_frames = es.MusicExtractor(lowlevelStats=['mean', 'stdev'],\n",
        "                                             rhythmStats=['mean', 'stdev'],\n",
        "                                             tonalStats=['mean', 'stdev'])(file)\n",
        "\n",
        "      for feature_name in features.descriptorNames():\n",
        "        if feature_name not in data.keys():\n",
        "          data[feature_name] = []\n",
        "        data[feature_name].append(features[feature_name])\n",
        "      data['name'].append(file.split('/')[-1])\n",
        "      data['labels'].append(i)\n",
        "      print(\"Extracted features for \", file)\n",
        "      if j == 2:\n",
        "        break\n",
        "  with open(json_save_file, 'w') as f:\n",
        "    json.dumps(data, f, indent=4)\n",
        "\n",
        "get_low_level_features('outfile')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}